{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Análise Exploratória de Dados\n\nA análise de dados permite que as empresas entendam sua eficiência e desempenho e, finalmente, ajuda a empresa a tomar decisões mais informadas. Por exemplo, uma empresa de comércio eletrônico pode ter interesse em analisar os atributos do cliente para exibir anúncios segmentados para melhorar as vendas. A análise de dados pode ser aplicada a quase qualquer aspecto de um negócio, se entendermos as ferramentas disponíveis para processar informações.\n\nA **Análise Exploratória de Dados** desempenha um papel crítico na compreensão do quê, por que e como na análise de um problema. É o primeiro passo na ordem de operações que um analista de dados executará ao entregar uma análise.\n\n> Aqui está uma definição simples: A análise exploratória de dados é uma abordagem para analisar conjuntos de dados, resumindo suas principais características com visualizações.\n\nO processo de AED é um passo crucial antes da construção de um modelo robusto. Esta exemplo apresenta os principais componentes da análise exploratória de dados, a fim de que você possa começar a analisar seus próprios dados. Abordaremos algumas explicações teóricas relevantes, bem como exemplos de código para que você possa aplicar essas técnicas ao seu próprio conjunto de dados.\n\n**Objetivos**\n* Ler e examinar um conjunto de dados\n* Classificar variáveis por tipo: quantitativo x categórico\n* Trabalhar com variáveis categóricas\n* Realize análises univariadas e bivariadas e obtenha insights significativos sobre o conjunto de dados\n* Identificar e tratar valores ausentes e discrepantes no conjunto de dados\n* Construir uma matriz de correlação para identificar variáveis relevantes\n\n## Historical Sales and Active Inventory\n\n### Descrição do problema:\n\n* **Contexto** - Uma empresa de varejo tem muitos produtos em seu inventário, e muito poucos deles tendem a vender (apenas cerca de 10% vendem a cada ano) e muitos dos produtos têm apenas uma única venda no curso de um ano.\n* **Objetivo** - A equipe de vendas e crescimento da empresa de varejo quer determinar quais produtos de seus estoques devem reter para vender e quais deles descartar\n* **Dados** - Todos os dados históricos de vendas e inventário ativo. link: [https://www.kaggle.com/flenderson/sales-analysis](https://www.kaggle.com/flenderson/sales-analysis)\n\n---\n\n**1º passo:** Vamos fazer o download do arquivo que será trabalhado..."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!curl https://fiapfunctions.blob.core.windows.net/datasets/SalesKaggle3.csv -o data/SalesKaggle3.csv",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib import pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Vamos analisar o conjunto de dados e examinar mais de perto seu conteúdo. O objetivo aqui é encontrar detalhes como o número de colunas e outros metadados que nos ajudarão a avaliar o tamanho e outras propriedades, como o intervalo de valores nas colunas do conjunto de dados.\n\nPara obter um pouco mais de contexto sobre os dados, é necessário entender o que as colunas significam em relação ao contexto do negócio - isso ajuda a estabelecer regras para as possíveis transformações que podem ser aplicadas aos valores da coluna.\n\nO ideial é sempre obter o data sheet dos dados. Nesse caso é fácil por estar documentado no Kaggle. Aqui estão as definições importantes das colunas:\n\n* **File_Type**:  O valor \"Active\" significa que o produto específico precisa de investigação\n* **SoldFlag**:  O valor 1 = venda, 0 = sem venda nos últimos seis meses\n* **SKU_number**:  Esse é o identificador exclusivo de cada produto.\n* **Order**:  Apenas um contador seqeencial. Pode ser ignorado.\n* **SoldFlag**:  1 = vendido nos últimos 6 meses. 0 = não vendido\n* **MarketingType**:  Neste caso se trata das duas categorias que definem como o produto é comercializado.\n* **New_Release_Flag**:  Qualquer produto que tenha uma versão futura caso esse número seja > 1."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data = pd.read_csv(\"data/SalesKaggle3.csv\")\nsales_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A função `describe()` retorna uma série de estatísticas descritivas que resumem a tendência central, dispersão e a forma da distribuição dos dados em questão. Vale notar que esta descrição não considera os valores NaN ou NULL. As três medidas numéricas principais para o centro de uma distribuição são o modo, a média (µ) e a mediana (M). \n\nO modo é o valor mais frequente. A média é o valor médio, enquanto a mediana é o valor central."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Quando chamamos a função `describe(include='all')`, forçamos a descrição de todas as colunas incluindo os valores **categóricos**."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data.describe(include='all')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Formato do dataframe\nprint(sales_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Número total de produtos e valores únicos das colunas\nprint(sales_data.nunique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Para nos mantermos na descrição do problema, precisamos considerar apenas os produtos ativos. Sendo assim primeiro vamos verificar a quantidade de valores históricos e ativos em nosso dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(sales_data[sales_data['File_Type'] == 'Historical']['SKU_number'].count())\nprint(sales_data[sales_data['File_Type'] == 'Active']['SKU_number'].count())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Já determinados o número de casos ativos e históricos. Temos 122921 casos ativos que precisam ser analisados. \n\nAgora vamoss dividir o dataset em duas partes com base no **File_Type**. Para tanto, vamos realizar uma `query` no dataframe, passando a condição necessária entre colchetes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data_hist = sales_data[sales_data['File_Type'] == 'Historical']\nsales_data_act = sales_data[sales_data['File_Type'] == 'Active']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1ª ETAPA\n\nNosso dataset contém **198.917** linhas e **14 colunas**, sendo 12 **numéricas** e 2 **categóricas**. Determinamos que existem **122.921** produtos vendidos ativamente. Estes são os dados onde vamos focar nossa análise.\n\n---\n\n**Contexto teórico**\n\n1. A **estatística Univariada** inclui todos os métodos de Estatística Descritiva que permitem a análise de cada variável separadamente e também métodos de Estatística Inferencial para determinada variável, podendo esta ser medida para uma ou mais amostras independentes\n2. A **estatística Bivariada** inclui métodos de análise de duas variáveis, podendo ser ou não estabelecida uma relação de causa/efeito entre elas.\n3. A **estatística Multivariada** inclui os métodos de análise das relações de múltiplas variáveis dependentes e/ou múltiplas variáveis independentes, quer se estabeleçam ou não relações de causa/efeito entre estes dois grupos. São também incluídos na Estatística Multivariada os métodos de análise das relações entre indivíduos caracterizados por duas ou mais variáveis.\n\n---\n\nOs dados associados a cada atributo incluem uma longa lista de valores, tanto numéricos quanto categóricos. Ter estes valores de forma serial não é particularmente útil, dado que eles geralmente não fornecem nenhum insight se analisados de forma autônoma. Para converter os dados brutos em informações que podemos usar, precisamos resumir e examinar a distribuição da variável.\n\nOs gráficos de distribuição univariada são gráficos onde plotamos os histogramas juntamente com a função de densidade de probabilidade estimada sobre os dados. É uma das técnicas mais simples, onde consideramos uma única variável e observamos suas propriedades estáticas e de propagação. A análise univariada para atributos numéricos e categóricos é diferente.\n\nPara os valores categóricos, podemos plotar histogramas. Se utilizando das funções value_count() e plot.bar(), é possível desenhar um gráfico simples de barra representando a contagem dos valores. Neste caso, temos dois tipos de marketing ` S ` e ` D `. O gráfico de barras mostra comparações entre essas categorias discretas, com o eixo x mostrando as categorias específicas e o eixo y o valor medido."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "sales_data['MarketingType'].value_counts().plot.bar(title=\"Freq dist of Marketing Type\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data['File_Type'].value_counts().plot.bar(title=\"Freq dist of File Type\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data['New_Release_Flag'].value_counts().plot.bar(title=\"Freq dist of New_Release_Flag\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data_act['MarketingType'].value_counts().plot.bar(title=\"Freq dist of MarketingFile Type - active states\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Contexto teórico**\n\n> Probability Density Function: PDF ou função de densidade de probabilidade em português, se refere a probabilidade de distribuição de uma variável aleatória contínua, em posição a uma variável aleatória discreta.\n\nEm termos estatísticos, quando representamos o PDF graficamente, a área sob a curva indica o intervalo em que a variável analisada irá cair. A área total neste intervalo do gráfico é igual à probabilidade de ocorrência de uma variável aleatória contínua.\n\nA função de densidade de probabilidade (PDF) não deve ser confundida com a função de massa de probabilidade (PMF). Esta última é aplicável a variáveis aleatórias discretas. A maneira mais fácil de diferenciar entre `variáveis discretas` versus `variáveis contínuas`, é que as discretas podem ser contadas (ou seja, o número é finito e normalmente assume a forma de um inteiro) e contínuas não (isto é, valores infinitos).\n\n---\n\nAgora podemos verificar a distribuição univariada das colunas numéricas que contém os histogramas e o PDF estimado.\n\nCom exceção da coluna `ReleaseYear`, todas as demais tem uma inclinação para a esquerda. Isso indica que a coluna `ReleaseYear` tem a maioria de seus valores no intervalo superior, enquanto todas as demais estão no intervalo inferior."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "col_names = ['StrengthFactor','PriceReg', 'ReleaseYear', 'ItemCount', 'LowUserPrice', 'LowNetPrice']\n\nfig, ax = plt.subplots(len(col_names), figsize=(16,20))\n\nfor i, col_val in enumerate(col_names):\n    sns.distplot(sales_data_hist[col_val], hist=True, ax=ax[i])\n    ax[i].set_xlabel(col_val, fontsize=8)\n    ax[i].set_ylabel('Count', fontsize=8)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A vantagem de utilizar gráficos de distribuição `bivariada`, é que eles nos ajudam a estudar a relação entre duas variáveis. Utilizar um gráfico de dispersão pode ser extremamente útil nesses casos.\n\nFrequentemente, procuramos nos gráficos de dispersão, aqueles que mostrem um padrão linear claro, com um declive crescente ou decrescente. \n\nNeste dataset em particular, não temos uma padrão claro de linearidade a ser investigado com maior cautela. Dito isso, sempre há espaço para derivar outros insights que possam ser úteis comparando a natureza dos gráficos entre as variáveis de interesse."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data_hist = sales_data_hist.drop(['Order', 'File_Type','SKU_number','SoldFlag','MarketingType','ReleaseNumber','New_Release_Flag'], axis=1)\nsns.pairplot(sales_data_hist)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Procurando valores ausentes\n\nValores `ausentes` ou `omissos`, referem-se àqueles campos que estão vazios ou sem valores atribuídos (NULL). Isso geralmente ocorre devido a erros de entrada dos dados, falhas que ocorrem nos processos de coleta e muitas vezes ao unir várias colunas de diferentes tabelas, onde algumas condições podem levar aos valores ausentes. \n\nExistem inúmeras maneiras com as quais os valores ausentes são tratados, os mais fáceis são substituir o valor ausente pela média, mediana, modo ou valor constante/determinístico (chegamos a este valor baseado no conhecimento do domínio). O outra alternativa é remover toda linha onde este erro ocorra."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# verificar se existem valores nulos no dataset\nsales_data.isnull().values.any()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# realiza a contagem dos números nulos no dataset\nsales_data.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Neste caso em específico vamos substituir estes valores nulos por 0. Após aplicar a transformação, podemos verificar novamente nosso dataset e observar que agora não temos mais dados nulos."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data['SoldFlag'].fillna(0, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data['SoldCount'].fillna(0, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sales_data.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Procurando por outliers\n\nUm outlier pode indicar um erro nos dados (como erros de digitação, medição, efeitos sazonais, etc.), o que geralmente nos conduz a ação de correção. \n\nDiversas vezes as ações tomadas são a correção (caso seja possível identificar), e a retirada do registro no dataset. Isso é realizado para que estes dados não afetem os calculos estatísticos, ou gerem distribuições enganosas na hora de obter insights dos dados, o que nos conduzirá a uma análise incorreta.\n\nVamos utilizar a visualização do `boxplot` para identificar os outliers...\n\n--- \n\n**Contexto teórico**\n\nO boxplot ou diagrama de caixa é uma ferramenta gráfica que permite visualizar a distribuição e valores discrepantes (outliers) dos dados, fornecendo assim um meio complementar para desenvolver uma perspectiva sobre o caráter dos dados. Além disso, o boxplot também é uma disposição gráfica comparativa.\n\nPara uma correta interpretação do boxplot, se faz necessário uma breve explicação sobre os quartis, que são as medidas estatísticas descritivas apresentadas no boxplot.\n\n**Quartis e Percentil:** O percentil é uma medida de posição que, dada uma amostra ordenada em ordem crescente e dividida em 100 partes, indica o valor do qual determinado percentual de elementos da amostra são menores ou iguais a ele. No geral trabalhamos com $1/4$ do todo, definindo percentis de 25, 50 e 75.\n\nOs quartis nada mais são que os percentis 25, 50 e 75, representando respectivamente o primeiro, segundo e terceiro quartil. O segundo quartil equivale ao percentil 50, valor em que pelo menos 50% da amostra está acima dele e pelo menos 50% está abaixo, ou seja, estamos falando da mediana.\n\n![bp](https://fiapfunctions.blob.core.windows.net/datasets/boxplot-1.png)\n\nReferência: [khanacademy.org/math/statistics-probability/identifying-outliers-iqr-rule](https://pt.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule)\n\n---\n\nCom base na definição acima, podemos concluir que as caixas coloridas são os itens na faixa de `IQR` (interquartile range), enquanto os pontos são nossos `outliers`."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "col_names = ['StrengthFactor','PriceReg', 'ReleaseYear', 'ItemCount', 'LowUserPrice', 'LowNetPrice']\n\nfig, ax = plt.subplots(len(col_names), figsize=(8,40))\n\nfor i, col_val in enumerate(col_names):\n    sns.boxplot(y=sales_data_hist[col_val], ax=ax[i])\n    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)\n    ax[i].set_xlabel(col_val, fontsize=8)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Tratando outliers\n\nO próximo passo natural após a identificação de `outliers` em nossos dados, é realizar o devido tratamento, ou seja, remover os valores discrepantes.\n\nUma das técnicas mais utilizadas é remover os valores atípicos com base no `percentil`. Existem outras técnicas como `z-score`, então não deixe de pesquisar :)\n\nOs valores marcados com um ponto abaixo do eixo x no gráfico, são aqueles que serão removidos com base no percentil definido (95 nesse caso)."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "def percentile_based_outlier(data, threshold=95):\n    diff = (100 - threshold) / 2\n    minval, maxval = np.percentile(data, [diff, 100 - diff])\n    return (data < minval) | (data > maxval)\n\ncol_names = ['StrengthFactor','PriceReg', 'ReleaseYear', 'ItemCount', 'LowUserPrice', 'LowNetPrice']\n\nfig, ax = plt.subplots(len(col_names), figsize=(8,40))\n\nfor i, col_val in enumerate(col_names):\n    x = sales_data_hist[col_val][:1000]\n    sns.distplot(x, ax=ax[i], rug=True, hist=False)\n    outliers = x[percentile_based_outlier(x)]\n    ax[i].plot(outliers, np.zeros_like(outliers), 'ro', clip_on=False)\n\n    ax[i].set_title('Outlier detection - {}'.format(col_val), fontsize=10)\n    ax[i].set_xlabel(col_val, fontsize=8)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Verificando a correlação\n\nUma matriz de correlação é uma tabela que mostra o valor do coeficiente de correlação, que são usados na estatística para medir a relação entre duas variáveis.\n\nCada atributo do conjunto de dados é comparado com os outros atributos para descobrir o coeficiente de correlação. Essa análise permite ver quais pares têm a correlação mais alta. Os pares que possuem um alto grau de correlação, representam a mesma variância do conjunto de dados, portanto, podemos analisá-los para entender qual atributo entre os pares é mais significativo para a construção do modelo.\n\nAbaixo temos o resultado da nossa matriz de correlação. O valores variam entre -1 e +1. \n\nAs variáveis altamente correlacionadas terão valor de correlação próximo a +1, e as variáveis com baixa correlação terão valor próximo a -1. Os valores diagonais são sempre 1, dado que falamos da correlação entre as mesmas variáveis.\n\nNeste exemplo em questão, não encontramos nenhuma correlação clara, o que nos ajuda a inferir que todos os atributos são importantes e devem ser considerados para a construção de um modelo."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(10, 8))\ncorr = sales_data_hist.corr()\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": ""
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}