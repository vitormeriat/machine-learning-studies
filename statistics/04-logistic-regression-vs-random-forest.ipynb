{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Versus Random Forest\n",
    "\n",
    "In this chapter, we will be making a comparison between logistic regression and random forest, with a classification example of German credit data. Logistic regression is a very popularly utilized technique in the credit and risk industry for checking the probability of default problems. Major challenges nowadays being faced by credit and risk departments with regulators are due to the black box nature of machine learning models, which is slowing down the usage of advanced models in this space. However, by drawing comparisons of logistic regression with random forest, some turnarounds could be possible; here we will discuss the variable importance chart and its parallels to the p-value of logistic regression, also we should not forget the major fact that significant variables remain significant in any of the models on a fair ground, though some change in variable significance always exists between any two models.\n",
    "\n",
    "## Maximum likelihood estimation\n",
    "\n",
    "Logistic regression works on the principle of maximum likelihood estimation; here, we will explain in detail what it is in principle so that we can cover some more fundamentals of logistic regression in the following sections. Maximum likelihood estimation is a method of estimating the parameters of a model given observations, by finding the parameter values that maximize the likelihood of making the observations, this means finding parameters that maximize the probability p of event 1 and (1-p) of non-event 0, as you know:\n",
    "\n",
    "probability (event + non-event) = 1\n",
    "\n",
    "Example: Sample (0, 1, 0, 0, 1, 0) is drawn from binomial distribution. What is the maximum likelihood estimate of µ?\n",
    "\n",
    "Solution: Given the fact that for binomial distribution P(X=1) = µ and P(X=0) = 1- µ where µ is the parameter:\n",
    "\n",
    "![01](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/e46a59bf-2f83-41aa-ad27-3c8b8cb915eb.jpg)\n",
    "\n",
    "Here, log is applied to both sides of the equation for mathematical convenience; also, maximizing likelihood is the same as the maximizing log of likelihood:\n",
    "\n",
    "![02](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/28343aef-1c43-4ce3-a175-5bded10e4b8e.jpg)\n",
    "\n",
    "Determining the maximum value of µ by equating derivative to zero:\n",
    "\n",
    "![03](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/c158d2b7-0d3b-4e85-b26e-ed7bfa8dae6c.jpg)\n",
    "\n",
    "However, we need to do double differentiation to determine the saddle point obtained from equating derivative to zero is maximum or minimum. If the µ value is maximum; double differentiation of log(L(µ)) should be a negative value:\n",
    "\n",
    "![04](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/9717d666-a66d-467a-ad6a-80e299267050.jpg)\n",
    "\n",
    "Even without substitution of µ value in double differentiation, we can determine that it is a negative value, as denominator values are squared and it has a negative sign against both terms. Nonetheless, we are substituting and the value is:\n",
    "\n",
    "![05](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/79bd2bc0-e73c-48c2-9c30-fcfbefe2c4af.jpg)\n",
    "\n",
    "Hence it has been proven that at value µ = 1/3, it is maximizing the likelihood. If we substitute the value in the log likelihood function, we will obtain:\n",
    "\n",
    "![06](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/382d5de9-af21-4291-81a1-2685fe8ff411.jpg)\n",
    "\n",
    "The reason behind calculating -2*ln(L) is to replicate the metric calculated in proper logistic regression. In fact:\n",
    "\n",
    "AIC = -2*ln(L) + 2*k\n",
    "\n",
    "So, logistic regression tries to find the parameters by maximizing the likelihood with respect to individual parameters. But one small difference is, in logistic regression, Bernoulli distribution will be utilized rather than binomial. To be precise, Bernoulli is just a special case of the binomial, as the primary outcome is only two categories from which all the trails are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression – introduction and advantages\n",
    "\n",
    "Logistic regression applies maximum likelihood estimation after transforming the dependent variable into a logit variable (natural log of the odds of the dependent variable occurring or not) with respect to independent variables. In this way, logistic regression estimates the probability of a certain event occurring. In the following equation, log of odds changes linearly as a function of explanatory variables:\n",
    "\n",
    "![07](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/82428344-dfb9-424b-8183-6dda83b832b9.jpg)\n",
    "\n",
    "One can simply ask, why odds, log(odds) and not probability? In fact, this is interviewers favorite question in analytics interviews.\n",
    "\n",
    "The reason is as follows:\n",
    "\n",
    "![08](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/c6213f86-2c3b-43ca-833c-bf96f8236330.jpg)\n",
    "\n",
    "By converting probability to log(odds), we have expanded the range from [0, 1] to [- ∞, +∞ ]. By fitting model on probability we will encounter a restricted range problem, and also by applying log transformation, we cover-up the non-linearity involved and we can just fit with a linear combination of variables.\n",
    "\n",
    "One more question one ask is what will happen if someone fit the linear regression on a 0-1 problem rather than on logistic regression?\n",
    "\n",
    "A brief explanation is provided with the following image:\n",
    "\n",
    "* Error terms will tend to be large at the middle values of X (independent variable) and small at the extreme values, which is the violation of linear regression assumptions that errors should have zero mean and should be normally distributed\n",
    "* Generates nonsensical predictions of greater than 1 and less than 0 at end values of X\n",
    "* The ordinary least squares (OLS) estimates are inefficient and standard errors are biased\n",
    "* High error variance in the middle values of X and low variance at ends\n",
    "\n",
    "All the preceding issues are solved by using logistic regression.\n",
    "\n",
    "![09](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/df28507c-8ace-4d2b-8328-99de9dba4ddb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology involved in logistic regression\n",
    "\n",
    "Logistic regression is favorite ground for many interviewers to test the depth of an analyst with respect to their statistical acumen. It has been said that, even if someone understands 1,000 concepts in logistic regression, there would always be a question 1,001 from an interviewer. Hence, it would really be worth building knowledge on logistic regression from its fundamentals in order to create a solid foundation:\n",
    "\n",
    "* Information value (IV): This is very useful in the preliminary filtering of variables prior to including them in the model. IV is mainly used by industry for eliminating major variables in the first step prior to fitting the model, as the number of variables present in the final model would be about 10. Hence, initial processing is needed to reduce variables from 400+ in number or so.\n",
    "\n",
    "![10](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/87778ca4-9ccf-465d-8d83-ca718aac0632.jpg)\n",
    "\n",
    "![11](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/c82ff9f6-23cd-4d83-ab0f-64d2716714e7.png)\n",
    "\n",
    "* Example: In the following table, continuous variable (price) has been broken down into deciles (10 bins) based on price range and the counted number of events and non-events in that bin, and the information value has been calculated for all the segments and added together. We got the total value as 0.0356, meaning it is a weak predictor to classify events.\n",
    "\n",
    "![12](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2e7c27de-c7d3-4387-8054-2f16366e3df3.png)\n",
    "\n",
    "* Akaike information criteria (AIC): This measures the relative quality of a statistical model for a given set of data. It is a trade-off between bias versus variance. During a comparison between two models, the model with less AIC is preferred over higher value.\n",
    "\n",
    "> If we closely observe the below equation, k parameter (the number of variables included in the model) is penalizing the overfitting phenomena of the model. This means that we can artificially prove the training accuracy of the model by incorporating more not so significant variables in the model; by doing so, we may get better accuracy on training data, but on testing data, accuracy will decrease. This phenomenon could be some sort of regularization in logistic regression:\n",
    "\n",
    "AIC = -2*ln(L) + 2*k \n",
    "\n",
    "L = Maximum value of Likelihood (log transformation applied for mathematical convenience)\n",
    "\n",
    "k = Number of variables in the model\n",
    "\n",
    "* Receiver operating characteristic (ROC) curve: This is a graphical plot that illustrates the performance of a binary classifier as its discriminant threshold is varied. The curve is created by plotting true positive rate (TPR) against false positive rate (FPR) at various threshold values.\n",
    "\n",
    "> A simple way to understand the utility of the ROC curve is that, if we keep the threshold value (threshold is a real value between 0 and 1, used to convert the predicted probability of output into class, as logistic regression predicts the probability) very low, we will put most of the predicted observations under the positive category, even when some of them should be placed under the negative category. On the other hand, keeping the threshold at a very high level penalizes the positive category, but the negative category will improve. Ideally, the threshold should be set in a way that trade-offs value between both categories and produces higher overall accuracy:\n",
    "\n",
    "Optimum threshold = Threshold where maximum (sensitivity + specificity) is possible\n",
    "\n",
    " > Before we jump into the nitty-gritty, we will visualize the confusion matrix to understand the various following formulas:\n",
    "\n",
    "![13](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/4e14add4-f378-4d0f-88af-9d02203a6a2d.png)\n",
    "\n",
    "![14](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/5bb6e8c3-023c-46a4-8476-9c0ca08bbfc6.jpg)\n",
    "\n",
    "![15](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/74d94ef1-c369-409a-939f-1ae690aa4e85.jpg)\n",
    "\n",
    "![16](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/e20b3fbf-8e35-42c7-bab4-9eacfec63760.jpg)\n",
    "\n",
    "![17](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/6240663c-6f7d-402f-8b61-fc640680cb53.jpg)\n",
    "\n",
    "![18](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/5b713619-dfff-4f8f-a003-85f80f897b37.jpg)\n",
    "\n",
    "![19](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/e42d7f42-929b-4a69-a781-18a505955767.jpg)\n",
    "\n",
    "The ROC curve will look as follows:\n",
    "\n",
    "![20](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/6c53f250-efff-408c-aa62-57382d80c4b1.png)\n",
    "\n",
    "* Rank ordering: After sorting observations in descending order by predicted probabilities, deciles are created (10 equal bins with 10 percent of total observations in each bin). By adding up the number of events in each decile, we will get aggregated events for each decile and this number should be in decreasing order, else it will be in serious violation of logistic regression methodology.\n",
    "\n",
    "> One way to think about why rank ordering is important? It will be very useful when we set the cut-off points at the top three to four deciles to send marketing campaigns where the segments have a higher chance of responding to the campaign. If rank order does not hold for the model, even after selecting the top three to four deciles, there will be a significant chunk left out below the cut-off point, which is dangerous.\n",
    "\n",
    "* Concordance/c-statistic: This is a measure of quality of fit for a binary outcome in a logistic regression model. It is a proportion of pairs in which the predicted event probability is higher for the actual event than non-event.\n",
    "\n",
    "* Example: In the following table, both actual and predicted values are shown with a sample of seven rows. Actual is the true category, either default or not; whereas predicted is predicted probabilities from the logistic regression model. Calculate the concordance value.\n",
    "\n",
    "![21](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/622c75ee-81d0-4758-9953-1f70c1335660.png)\n",
    "\n",
    "For calculating concordance, we need to split the table into two (each table with actual values as 1 and 0) and apply the Cartesian product of each row from both tables to form pairs:\n",
    "\n",
    "![22](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/f48c691d-e6bb-4f2b-9dc2-e004e4dd3c3e.png)\n",
    "\n",
    "In the following table, the complete Cartesian product has been calculated and has classified the pair as a concordant pair whenever the predicted probability for 1 category is higher than the predicted probability for 0 category. If it is the other way around, then the pair has been classified as a discordant pair. In special cases, if both probabilities are the same, those pairs will be classified as tied instead.\n",
    "\n",
    "![23](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/91a06e7e-866c-4492-9df2-149edcf9ef32.png)\n",
    "\n",
    "![24](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/5ac017c9-54a2-48d4-a906-2d79186daf56.jpg)\n",
    "\n",
    "![25](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/c438bb45-94f2-44b6-9953-eba092b73ea3.jpg)\n",
    "\n",
    "![26](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/55178156-71cb-485f-9f98-39641de7d6a4.jpg)\n",
    "\n",
    "![27](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/409a3213-cae6-428a-adeb-766756482e4e.jpg)\n",
    "\n",
    "![28](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/552d037d-5e8c-4322-9fd1-5de1ab57c486.jpg)\n",
    "\n",
    "* C-statistic: This is 0.83315 percent or 83.315 percent, and any value greater than 0.7 percent or 70 percent is considered a good model to use for practical purposes.\n",
    "\n",
    "* Divergence: The distance between the average score of default accounts and the average score of non-default accounts. The greater the distance, the more effective the scoring system is at segregating good and bad observations.\n",
    "\n",
    "* K-S statistic: This is the maximum distance between two population distributions. It helps with discriminating default accounts from non-default accounts.\n",
    "\n",
    "* Population stability index (PSI): This is the metric used to check that drift in the current population on which the credit scoring model will be used is the same as the population with respective to development time:\n",
    "    * PSI <= 0.1: This states no change in characteristics of the current population with respect to the development population\n",
    "    * 0.1 < PSI <= 0.25: This signifies some change has taken place and warns for attention, but can still be used\n",
    "    * PSI >0.25: This indicates a large shift in the score distribution of the current population compared with development time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
