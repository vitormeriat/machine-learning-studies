{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"width: 100%; background-color: #F7F7F7; margin-bottom: 3em;\">\n",
    "    <h1 style=\"font-family: courier; color: #AF4545; font-size: 4em; padding-top: .5em;\"><b>Statistical fundamentals and terminology for model building and validation</b></h1>\n",
    "    <img src=\"../img/statistics-head.png\">    \n",
    "</div>\n",
    "\n",
    "Este estudo é baseado no livro `Statistics for Machine Learning`, e o código abaixo possui trechos integrais do livro. Dito isso existem algumas adaptações, sugestões e complementos que jugo necessários.\n",
    "\n",
    "---\n",
    "\n",
    "Par iniciar vou importar os pacotes necessários na primeira parte do estudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é uma introdução básica, a estatística é um tema para uma vida quanto mais um notebook. Com isso em mente, vou me ater aos principais pontos, os necessários para um bom conhecimento do universo do `Machine Learning`.\n",
    "\n",
    "> A análise preditiva se baseia na suposição de que a história tende a se repetir.\n",
    "\n",
    "Neste caso, para fazermos predições, nosso objetivo será ajustar um **modelo matemático** aos dados históricos validando suas principais medidas, de tal maneira que este modelo possa ser utilizado a fim de prever eventos futuros baseados nas mesmas **variáveis explicativas** que estudamos no passado.\n",
    "\n",
    "Modelos estatísticos são uma classe de modelos matemáticos que geralmente são especificados por equações matemáticas que relacionam uma ou mais variáveis a fim de se aproximar a realidade. Pressupostos incorporados por modelos estatísticos descrevem um conjunto de distribuições de probabilidade, que o diferencia dos modelos não estatísticos, matemáticos ou de aprendizado de máquina.\n",
    "\n",
    "Os modelos estatísticos sempre começam com algumas suposições subjacentes para as quais todas as variáveis devem se manter; então, o desempenho fornecido pelo modelo é estatisticamente significativo. Portanto, conhecer os vários bits envolvidos em todos os blocos de construção fornece uma base sólida para ser um bom cientista de dados.\n",
    "\n",
    "* **População (Population)**: é a totalidade, a lista completa de observações ou todos os pontos de dados sobre o assunto em estudo.\n",
    "* **Amostra (Sample)**: uma amostra é um subconjunto de uma população, geralmente uma pequena parte da população que está sendo analisada.\n",
    "\n",
    "Geralmente, é caro realizar uma análise em uma população inteira; portanto, a maioria dos métodos estatísticos trata de tirar conclusões sobre uma população analisando uma amostra.\n",
    "\n",
    "* **Parâmetro versus estatística (Parameter versus statistic)**: qualquer medida calculada na população é um parâmetro, enquanto que em uma amostra é chamada de estatística.\n",
    "* **Média (Mean)**: Esta é uma média aritmética simples, calculada pela soma agregada dos valores dividida por uma contagem desses valores. A média é sensível aos valores discrepantes nos dados. Um valor externo é o valor de um conjunto ou coluna que é altamente desviado de muitos outros valores nos mesmos dados; geralmente tem valores muito altos ou baixos.\n",
    "* **Mediana (Median)**: é o ponto médio dos dados e é calculado organizando-os em ordem crescente ou decrescente. Se houver N observações.\n",
    "* **Moda (Mode)**: é o ponto mais repetitivo nos dados:\n",
    "\n",
    "![0](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d7ca0b32-24a6-43fd-8281-3f995895ff1b.png)\n",
    "\n",
    "O código abaixo usa o pacote stats e numpy para calular a média (median), mediana (median) e moda (mode).\n",
    "\n",
    "A matriz inicial foi criada utilizando o numpy, isso dado que os modelos estatísticos utilizados neste estudo serão criados utilizando o pacote scikit-learn, que é baseado no numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 4.33\n",
      "Median : 4.0\n",
      "Mode : 2\n"
     ]
    }
   ],
   "source": [
    "data = np.array([4,5,1,2,7,2,6,9,3])\n",
    "\n",
    "# Calculate Mean\n",
    "dt_mean = np.mean(data)\n",
    "print (\"Mean :\",round(dt_mean,2))\n",
    "              \n",
    "# Calculate Median                 \n",
    "dt_median = np.median(data)\n",
    "print (\"Median :\",dt_median)\n",
    "\n",
    "# Calculate Mode                     \n",
    "dt_mode =  stats.mode(data)\n",
    "print (\"Mode :\",dt_mode[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuições de probabilidade\n",
    "\n",
    "As principais distribuições de probabilidade estão implementadas na biblioteca `scipy.stats`. Todas as distribuições implementadas nesta biblioteca tem um conjunto rico de métodos os principais são:\n",
    "\n",
    "  - **pdf**: função densidade probabilidade ou função de probabilidade.\n",
    "  - **cdf**: função de distribuição acumulada.\n",
    "  - **sf**: função de sobreviência (complementar da cdf).\n",
    "  - **ppf**: função quantil (inversa da cdf).\n",
    "  - **isf**: inversa da função de sobreviência (complementar do inverso da cdf).\n",
    "  - **stats**: esperança, variância, assimetria (skew) e curtose.\n",
    "  - **moment**: Momentos não centrais.\n",
    "  - **rvs**: amostras aleatórias.\n",
    "\n",
    "Interessante notar que funções como a pdf e cdf são definidas na reta real, mesmo que a distribuição não corresponda a este suporte. Neste caso as funções vão retornar zero no caso da pdf e 0 ou 1 no caso da cdf. Por exemplo, o suporta da distribuição beta é o intervalo aberto (0,1). Assim, se a pdf fora deste intervalo será 0 e a cdf abaixo de 0 será 0 e acima de 1 será 1. \n",
    "\n",
    "Outro aspecto interessante da forma como a biblioteca `scipy.stats` implementa as distribuições é que você pode usá-las de duas formas: a primeira é a chamada forma congelada onde você inicializa a distribuição com os parâmetros de interesse e depois usa. A segunda você aplica a função diretamente passando como argumentos o ponto e os parâmetros para a avaliação da função. Vamos ver um exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08801633169107488\n",
      "0.08801633169107488\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as sp\n",
    "\n",
    "print(sp.norm.pdf(5, 3, 4)) # Avalia a distribuição Normal com mu = 3 e sigma2 = 4 no ponto 5.\n",
    "\n",
    "mydist = sp.norm(3, 4) # Modo frozen \n",
    "print(mydist.pdf(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca `scipy.stats` é muito rica em distribuições e a documentação é muito detalhada recomendo que veja o site da biblioteca [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html). Um outro aspecto útil é que todas as distribuições contínuas estão implementadas como um modelo de locação e escala. Esta é uma abordagem um pouco não usual, mas uma grande vantagem é que sabemos a parametrização de todas as distribuições contínuas previamente. A lista de distribuições é muito extensa e inclui distribuições multivariadas como a Gaussiana, Dirichlet e Wishart. \n",
    "\n",
    "Como um exemplo para explorar como usamos as distribuições da `scipy.stats` vou usar em um primeiro momento a distribuição Gaussiana porque suas propriedades são bem conhecidas. Em um segundo momento fazer usar uma distribuição não usual como a *generalized extreme value distribution*. Vamos calcular algumas quantidades da distribuição Gaussiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 25.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "my_norm = sp.norm(loc = 10, scale = 5)\n",
    "# Aspectos da distribuição\n",
    "my_norm.expect() # Esperança\n",
    "my_norm.median() # Mediana\n",
    "my_norm.moment(n = 1) # Primeiro momento (esperança)\n",
    "my_norm.moment(n = 2) # Segundo momento\n",
    "my_norm.stats() # Média e variancia\n",
    "my_norm.std() # Erro padrão\n",
    "my_norm.var() # Variance\n",
    "my_norm.entropy() # Entropia\n",
    "\n",
    "mean, var, skew, kurt = my_norm.stats(moments = 'mvsk')\n",
    "print(mean, var, skew, kurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda não falamos sobre gráficos em python, mas eu vou inicializar uma figura, mais detalhes serão vistos nos próximos encontros. Gráfico da função densidade probabilidade.\n",
    "\n",
    "Vamos simular uma amostra aleatório da distribuição Gaussiana e fazer um histograma e sobrepor a densidade que acabamos de calcular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "x = np.linspace(my_norm.ppf(0.01), my_norm.ppf(0.99), 100)\n",
    "ax.plot(x, my_norm.pdf(x), 'r-', lw=1, alpha=0.6, label='norm pdf')\n",
    "\n",
    "amostra = my_norm.rvs(10000)\n",
    "ax.hist(amostra, density=True, histtype='stepfilled', alpha=0.2)\n",
    "ax.legend(loc='best', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro método que também está disponível para todas as distribuições implementadas na biblioteca `scipy.stats` é o método `fit`. Como o nome sugere este método ajusta a distribuição para um vetor de observações. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.708235276775888, 4.69668219232939)\n",
      "(5349716.87731085, 9.708343871447159, 4.696777047664349)\n",
      "(9.2304936802949, 2.957632948450315)\n"
     ]
    }
   ],
   "source": [
    "amostra = my_norm.rvs(100)\n",
    "print(sp.norm.fit(amostra)) # Ajustando a própria Normal\n",
    "print(sp.t.fit(amostra)) # Ajustando a distribuição t\n",
    "print(sp.cauchy.fit(amostra)) # Cauchy ou qq outra que seja adequada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em geral as distribuições de probabilidade implementadas na biblioteca `scipy.stats` são vetorizadas. Assim, se o argumento for um objeto `ndarray` a função será aplicada em cada uma das entradas do `ndarray` e a saída também será um `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00443185, 0.05399097, 0.24197072, 0.39894228, 0.24197072,\n",
       "       0.05399097, 0.00443185])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([-3,-2,-1,0,1,2,3])\n",
    "sp.norm.pdf(a, loc = 0, scale = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim termino esta rápida introdução de como e onde as funções de probabilidades estão implementadas em python. Na sequência vou introduzir as principais idéias de otimização de funções em python.\n",
    "\n",
    "## Otimização\n",
    "\n",
    "Otimização de funções tem um papel central em inferência estatística. Uma vez que o estimador de maxima verossimilhança é o supremo da função de log-verossimilhança encontrar este ponto é crucial para o processo de inferência. O python através do modulo `scipy.optimization` fornece vários algoritmos para numericamente encontrar o mínimo/máximo de uma função pré-especificada. Este modulo também fornece algoritmos para solução de sistemas de equações não-lineares e minimização via métodos dos mínimos quadrados. Vamos ver alguns exemplos de como usar esta poderosa biblioteca. Como exemplo vou implementar a log-verossimilhança de um modelo de regressão linear simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.51650241 3.09539161 2.94864942 4.15234306 5.68080283 5.22924572\n",
      " 5.86809571 7.25463115 8.67303766 8.91618494]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(range(0,10))\n",
    "par = np.array([2,0.8,0.5])\n",
    "my_norm = sp.norm(loc = 0, scale = par[2])\n",
    "e = my_norm.rvs(len(x))\n",
    "mu = par[0] + par[1]*x\n",
    "y = mu + e\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.08257572  0.74465029 -0.73975402]\n",
      "6.792110640676109\n",
      "[ 2.0825901   0.74464408 -0.73972768]\n",
      "6.792110622630275\n",
      "[ 2.08259022  0.74464405 -0.7397274 ]\n",
      "6.792110622630187\n",
      "[ 2.08225433  0.7446971  -0.74038164]\n",
      "6.792115621087034\n",
      "[-3.57627869e-07 -8.28504562e-06  1.54972076e-06]\n",
      "[[ 0.0875535  -0.01430076 -0.00244704]\n",
      " [-0.01430076  0.0031595   0.00045238]\n",
      " [-0.00244704  0.00045238  0.05374031]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from math import exp, log\n",
    "\n",
    "def linreg(par, y, x):\n",
    "    mu = par[0] + par[1]*x\n",
    "    output = -sp.norm.logpdf(y, loc = mu, scale = exp(par[2])).sum()\n",
    "    #print(output)\n",
    "    return(output)\n",
    "\n",
    "# Avaliando a log-lik no ponto\n",
    "linreg(par = np.array([2.1788,0.7743, log(0.3817)]), y = y, x = x) \n",
    "\n",
    "# Valores iniciais\n",
    "par = np.array([2,0.8, log(2)])\n",
    "\n",
    "# Nelder-Mead\n",
    "res1 = minimize(linreg, par, method='nelder-mead', args = (y,x))\n",
    "print(res1.x)\n",
    "print(res1.fun)\n",
    "\n",
    "# Gradiente Conjugado\n",
    "res2 = minimize(linreg, par, method = 'CG', args = (y,x))\n",
    "print(res2.x)\n",
    "print(res2.fun)\n",
    "\n",
    "# BFGS\n",
    "res3 = minimize(linreg, par, method = 'BFGS',  args = (y,x))\n",
    "print(res3.x)\n",
    "print(res3.fun)\n",
    "\n",
    "# Powell\n",
    "res4 = minimize(linreg, par, method = 'Powell', args = (y,x))\n",
    "print(res4.x)\n",
    "print(res4.fun)\n",
    "\n",
    "# Escore\n",
    "print(res3.jac)\n",
    "\n",
    "# Temos o inverso do hessiano\n",
    "print(res3.hess_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Medida de dispersão ou variabilidade (Measure of variation)**: Dispersão é a variação nos dados e mede as inconsistências no valor das variáveis nos dados. A dispersão, na verdade, fornece uma idéia sobre o spread, em vez de valores centrais.\n",
    "* **Faixa (Range)**: Essa é a diferença entre o máximo e o mínimo do valor.\n",
    "* **Variância (Variance)**: é a média dos desvios ao quadrado da média ($xi$ = pontos de dados, $µ$ = média dos dados, $N$ = número de pontos de dados). A dimensão da variação é o quadrado dos valores reais. A razão para usar o denominador $N-1$ para uma amostra em vez de $N$ na população é devido ao grau de liberdade. 1 grau de liberdade perdido em uma amostra no momento do cálculo da variação é devido à extração da substituição da amostra:\n",
    "\n",
    "![1](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/04829f06-fff4-4694-9b3e-62508ac4a609.jpg)\n",
    "\n",
    "* **Desvio padrão (Standard deviation)**: esta é a raiz quadrada da variação. Aplicando a raiz quadrada na variação, medimos a dispersão em relação à variável original e não ao quadrado da dimensão:\n",
    "\n",
    "![2](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/ba61b3ca-aaf4-48cb-98a8-6f7acd61fef8.jpg)\n",
    "\n",
    "* **Quantiles (Quantiles)**: Estes são simplesmente fragmentos idênticos dos dados. Os quantiles cobrem percentis, decis, quartis e assim por diante. Essas medidas são calculadas após a organização dos dados em ordem crescente:\n",
    "    * **Percentil (Percentile)**: nada mais é do que a porcentagem de pontos de dados abaixo do valor dos dados completos originais. A mediana é o 50º percentil, pois o número de pontos de dados abaixo da mediana é de cerca de 50% dos dados.\n",
    "    * **Decil (Decile)**: este é o 10º percentil, o que significa que o número de pontos de dados abaixo do decil é 10% de todos os dados.\n",
    "    * **Quartil (Quartile)**: este é um quarto dos dados e também o 25º percentil. O primeiro quartil é 25% dos dados, o segundo quartil é 50% dos dados, o terceiro quartil é 75% dos dados. O segundo quartil também é conhecido como mediana ou percentil 50 ou quinto decil.\n",
    "    * **Intervalo interquartil (Interquartile range)**: é a diferença entre o terceiro quartil e o primeiro quartil. É eficaz na identificação de valores discrepantes nos dados. O intervalo interquartil descreve os 50% do meio dos pontos de dados.\n",
    "\n",
    "![3](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d2d6d978-262e-41fb-a731-41f8b8384d9b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample variance: 400\n",
      "Sample std.dev: 20.0\n",
      "Range: 69\n"
     ]
    }
   ],
   "source": [
    "# Deviance calculations\n",
    "\n",
    "from statistics import variance, stdev\n",
    "\n",
    "game_points = np.array([35,56,43,59,63,79,35,41,64,43,93,60,77,24,82])\n",
    "\n",
    "# Calculate Variance\n",
    "dt_var = variance(game_points)\n",
    "print (\"Sample variance:\", round(dt_var,2))\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "dt_std = stdev(game_points)\n",
    "print (\"Sample std.dev:\",round(dt_std,2))\n",
    "               \n",
    "# Calculate Range\n",
    "dt_rng = np.max(game_points,axis=0) - np.min(game_points,axis=0)\n",
    "print (\"Range:\",dt_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles:\n",
      "20% 39.800000000000004\n",
      "80% 77.4\n",
      "100% 93.0\n",
      "Inter quartile range: 28.5\n"
     ]
    }
   ],
   "source": [
    "#Calculate percentiles\n",
    "print (\"Quantiles:\")\n",
    "\n",
    "for val in [20,80,100]:\n",
    "    dt_qntls = np.percentile(game_points,val) \n",
    "    print (str(val)+\"%\" ,dt_qntls)\n",
    "                                \n",
    "# Calculate IQR                           \n",
    "q75, q25 = np.percentile(game_points, [75 ,25])\n",
    "print (\"Inter quartile range:\",q75-q25 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Teste de hipótese (Hypothesis testing)**: Este é o processo de fazer inferências sobre a população em geral, realizando alguns testes estatísticos em uma amostra. Hipóteses nulas e alternativas são formas de validar se uma suposição é estatisticamente significativa ou não.\n",
    "* **Valor p (P-value)**: a probabilidade de obter um resultado estatístico de teste é pelo menos tão extrema quanto a que foi realmente observada, assumindo que a hipótese nula seja verdadeira (geralmente na modelagem, em relação a cada variável independente, um valor p menor que 0,05 é considerados significativos e maiores que 0,05 são considerados insignificantes; no entanto, esses valores e definições podem mudar em relação ao contexto).\n",
    "\n",
    "As etapas envolvidas no teste de hipóteses são as seguintes:\n",
    "\n",
    "1. Suponha uma hipótese nula (geralmente sem diferença, sem significância, e assim por diante; uma hipótese nula sempre tenta assumir que não há padrão de anomalia e é sempre homogêneo, e assim por diante).\n",
    "2. Colete a amostra.\n",
    "3. Calcule as estatísticas de teste da amostra para verificar se a hipótese é estatisticamente significativa ou não.\n",
    "4. Decida aceitar ou rejeitar a hipótese nula com base na estatística do teste.\n",
    "\n",
    "* **Exemplo de teste de hipótese (Example of hypothesis testing)**: Um fabricante de chocolate que também é seu amigo afirma que todos os chocolates produzidos em sua fábrica pesam pelo menos 1.000 ge você tem uma sensação engraçada de que isso pode não ser verdade; vocês dois coletaram uma amostra de 30 chocolates e descobriram que o peso médio de chocolate é de 990 g, com desvio padrão da amostra de 12,5 g. Dado o nível de significância de 0,05, podemos rejeitar a reivindicação feita por seu amigo?\n",
    "\n",
    "A hipótese nula é que $µ0 ≥ 1000$ (todos os chocolates pesam mais de 1.000 g).\n",
    "\n",
    "Amostra coletada:\n",
    "\n",
    "![4](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/aa23714a-a55f-48ae-8103-22298c05c866.jpg)\n",
    "\n",
    "Teste estatístico:\n",
    "\n",
    "![5](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/696917cc-a126-4477-9e7d-df420f914a90.jpg)\n",
    "\n",
    "$t = (990 - 1000) / (12,5 / sqrt (30)) = - 4,3818$\n",
    "\n",
    "Valor t crítico das tabelas $t$ = $t0,05, 30 = 1,699 => - t0,05, 30 = -1,699$\n",
    "\n",
    "Valor P = 7,03 e-05\n",
    "\n",
    "A estatística do teste é -4,3818, que é menor que o valor crítico de -1,699. Portanto, podemos rejeitar a hipótese nula (afirmação de seu amigo) de que o peso médio de um chocolate está acima de 1.000 g.\n",
    "\n",
    "Além disso, outra maneira de decidir a reivindicação é usar o valor-p. Um valor p menor que 0,05 significa que os valores reivindicados e os valores médios da distribuição são significativamente diferentes; portanto, podemos rejeitar a hipótese nula:\n",
    "\n",
    "![6](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/fd9dea23-5c21-49c8-b363-623f43621e61.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic: -4.38\n",
      "Critical value from t-table: -1.699\n",
      "Lower tail p-value from t-table 7.035025729010886e-05\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis testing        \n",
    "\n",
    "xbar = 990; mu0 = 1000; s = 12.5; n = 30\n",
    "\n",
    "# Test Statistic\n",
    "t_smple  = (xbar-mu0)/(s/np.sqrt(float(n)))\n",
    "print (\"Test Statistic:\",round(t_smple,2))\n",
    "\n",
    "# Critical value from t-table\n",
    "alpha = 0.05\n",
    "t_alpha = stats.t.ppf(alpha,n-1)\n",
    "print (\"Critical value from t-table:\",round(t_alpha,3))\n",
    "\n",
    "#Lower tail p-value from t-table                        \n",
    "p_val = stats.t.sf(np.abs(t_smple), n-1)\n",
    "print (\"Lower tail p-value from t-table\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Erro tipo I e II (Type I and II error)**: O teste de hipóteses geralmente é realizado nas amostras e não em toda a população, devido às restrições práticas dos recursos disponíveis para coletar todos os dados disponíveis. No entanto, a realização de inferências sobre a população a partir de amostras tem seus próprios custos, como rejeitar bons resultados ou aceitar resultados falsos, sem mencionar separadamente, quando aumentos no tamanho da amostra levam a minimizar erros do tipo I e II:\n",
    "    * **Erro tipo I**: rejeitando uma hipótese nula quando verdadeira\n",
    "    * **Erro tipo II**: aceitando uma hipótese nula quando falsa\n",
    "* **Distribuição normal (Normal distribution)**: isso é muito importante na estatística por causa do teorema do limite central, que afirma que a população de todas as amostras possíveis de tamanho n de uma população com μ2 e variação σ2 próximas de uma distribuição normal:\n",
    "\n",
    "![7](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/f2dd7f75-6394-4238-9d6b-9fe7f1f52c31.jpg)\n",
    "\n",
    "Exemplo: Suponha que as notas dos testes de um exame de admissão se ajustem a uma distribuição normal. Além disso, a pontuação média no teste é 52 e o desvio padrão é 16,3. Qual é a porcentagem de alunos com 67 ou mais notas no exame?\n",
    "\n",
    "![08](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/0e2e6317-da80-44bf-80f6-4df886b67803.jpg)\n",
    "![09](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/59ccd175-1fd1-4cc5-87b9-e0ebf4023c9b.jpg)\n",
    "![10](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d1d90969-0e9d-4d4f-8381-4f8a1b634ee3.jpg)\n",
    "![11](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/11cf9613-e4b5-4bac-8056-1f1a0967030f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob. to score more than 67 is  17.87 %\n"
     ]
    }
   ],
   "source": [
    "# Normal Distribution\n",
    "\n",
    "xbar = 67; mu0 = 52; s = 16.3\n",
    "\n",
    "# Calculating z-score\n",
    "z = (67-52)/16.3\n",
    "\n",
    "# Calculating probability under the curve    \n",
    "p_val = 1- stats.norm.cdf(z)\n",
    "\n",
    "print (\"Prob. to score more than 67 is \",round(p_val*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Qui-quadrado (Chi-square)**: Esse teste de independência é um dos testes de hipótese mais básicos e comuns na análise estatística de dados categóricos. Dadas duas variáveis aleatórias categóricas $X$ e $Y$, o teste do qui-quadrado da independência determina se existe ou não uma dependência estatística entre elas.\n",
    "\n",
    "O teste geralmente é realizado calculando $χ2$ a partir dos dados e $χ2$ com (m-1, n-1) graus da tabela. É tomada uma decisão sobre se as duas variáveis são independentes com base no valor real e no valor da tabela, o que for maior:\n",
    "\n",
    "![11](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/c77913b4-32d4-4cf5-9474-a9e54f7837cc.jpg)\n",
    "\n",
    "Exemplo: Na tabela a seguir, calcule se o hábito de fumar tem impacto no comportamento do exercício:\n",
    "\n",
    "![12](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/1abc30fb-a6df-4091-aa10-2647446f98eb.png)\n",
    "\n",
    "Ao criar uma tabela usando a função de `crosstab` (tabela de referência cruzada), obteremos os campos de totais de linha e coluna extra. No entanto, para criar a tabela observada, precisamos extrair a parte das variáveis e ignorar os totais.\n",
    "\n",
    "```python\n",
    "observed = survey_tab.ix[0:4,0:3]  \n",
    "```\n",
    "\n",
    "A função `chi2_contingency` do pacote stats usa a tabela observada e subsequentemente calcula sua tabela esperada, seguida pelo cálculo do valor-p para verificar se duas variáveis são dependentes ou não. Se o valor de p for < 0,05, existe uma forte dependência entre duas variáveis, enquanto que, se o valor de p for > 0,05, não há dependência entre as variáveis:\n",
    "\n",
    "```python\n",
    "contg = stats.chi2_contingency(observed= observed) \n",
    "p_value = round(contg[1],3)\n",
    "```\n",
    "O valor de p é 0.483, o que siginifica que não há dependência entre o hábito de fumar e o comportamento do exercício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value is:  0.483\n"
     ]
    }
   ],
   "source": [
    "# Chi-square independence test\n",
    "import pandas as pd\n",
    "\n",
    "survey = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/survey.csv\")\n",
    "\n",
    "# Tabulating 2 variables with row & column variables respectively\n",
    "survey_tab = pd.crosstab(survey.Smoke, survey.Exer, margins = True)\n",
    "\n",
    "# Creating observed table for analysis\n",
    "observed = survey_tab.iloc[0:4,0:3] \n",
    "\n",
    "contg = stats.chi2_contingency(observed= observed)\n",
    "p_value = round(contg[1],3)\n",
    "\n",
    "print (\"P-value is: \",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **ANOVA**: A análise de variância testa a hipótese de que as médias de duas ou mais populações são iguais. As ANOVAs avaliam a importância de um ou mais fatores comparando as médias das variáveis de resposta nos diferentes níveis de fatores. A hipótese nula afirma que todas as médias populacionais são iguais, enquanto a hipótese alternativa afirma que pelo menos uma é diferente.\n",
    "\n",
    "Exemplo: Uma empresa de fertilizantes desenvolveu três novos tipos de fertilizantes universais após pesquisas que podem ser utilizadas para cultivar qualquer tipo de colheita. Para descobrir se os três têm um rendimento similar, eles escolheram aleatoriamente seis tipos de cultivo no estudo. De acordo com o delineamento em blocos ao acaso, cada tipo de cultura será testado com os três tipos de fertilizante separadamente. A tabela a seguir representa o rendimento em $g/m^2$. No nível de significância de 0,05, teste se os rendimentos médios para os três novos tipos de fertilizantes são todos iguais:\n",
    "\n",
    "| Fertilizer 1| Fertilizer 2 | Fertilizer 3 |\n",
    "| --- | --- | --- |\n",
    "| 62 | 54 | 48 |\n",
    "| 62 | 56 | 62 |\n",
    "| 90 | 58 | 92 |\n",
    "| 42 | 36 | 96 |\n",
    "| 84 | 72 | 92 |\n",
    "| 64 | 34 | 80 |\n",
    "\n",
    "Resultado: o valor de p foi menor que 0,05, portanto, podemos rejeitar a hipótese nula de que a produção média dos fertilizantes é igual. Os fertilizantes fazem uma diferença significativa para as culturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic : 3.66 , p-value : 0.051\n"
     ]
    }
   ],
   "source": [
    "#ANOVA\n",
    "\n",
    "fetilizers = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/fetilizers.csv\")\n",
    "\n",
    "one_way_anova = stats.f_oneway(fetilizers[\"fertilizer1\"], fetilizers[\"fertilizer2\"], fetilizers[\"fertilizer3\"])\n",
    "\n",
    "print (\"Statistic :\", round(one_way_anova[0],2),\", p-value :\",round(one_way_anova[1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Matriz de confusão (Confusion matrix)**: esta é a matriz do real versus o previsto. Este conceito é melhor explicado com o exemplo de previsão de câncer usando o modelo:\n",
    "\n",
    "![13](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/f512942f-24dd-47b7-8b28-f9ce86376587.png)\n",
    "\n",
    "Alguns termos usados em uma matriz de confusão são:\n",
    "\n",
    "* **Verdadeiros positivos (True positives > TPs)**: Verdadeiros positivos são casos em que predizemos a doença e sim quando o paciente realmente tem a doença.\n",
    "* **Negativos verdadeiros (True negatives > TNs)**: Casos em que predizemos a doença como não, quando o paciente realmente não tem a doença.\n",
    "* **Falsos positivos (False positives > PPs)**: quando prevemos a doença como sim, quando o paciente realmente não tem a doença. Os FPs também são considerados erros do tipo I.\n",
    "* **Falsos negativos (False negatives > FNs)**: quando prevemos a doença como não, quando o paciente realmente a tem. Os FNs também são considerados erros do tipo II.\n",
    "* **Precisão (Precision > P)**: Quando o previsto é sim, com que frequência é correto?\n",
    "\n",
    "$(TP / TP + FP)$\n",
    "\n",
    "* **Recall (R)/sensibilidade/taxa positiva verdadeira (Recall (R)/sensitivity/true positive rate)**: Entre os sim reais, que fração foi prevista como sim?\n",
    "\n",
    "$(TP / TP + FN)$\n",
    "\n",
    "* **F1 score (F1)**: Esta é a média harmônica da precisão e recuperação. Multiplicar a constante de 2 escala a pontuação para 1 quando a precisão e a recuperação são 1:\n",
    "\n",
    "![14](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/41e2c94c-d6c9-4227-81ac-d806f07c64f4.jpg)\n",
    "\n",
    "* **Especificidade (Specificity)**: Entre os números reais, que fração foi prevista como não? Também equivalente a 1 - taxa de falsos positivos:\n",
    "\n",
    "$(TN / TN + FP)$\n",
    "\n",
    "* **Área sob curva (Area under curve > ROC)**: a curva característica operacional do receptor é usada para traçar entre a taxa positiva verdadeira (TPR) e a taxa positiva falsa (FPR), também conhecido como gráfico de sensibilidade e 1- especificidade:\n",
    "\n",
    "![15](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/97f53598-dd30-406c-9106-b193d6844666.png)\n",
    "\n",
    "A área sob curva é utilizada para definir o limiar da probabilidade de corte para classificar a probabilidade prevista em várias classes; abordaremos como esse método funciona nos próximos capítulos.\n",
    "\n",
    "* **Janela de observação e desempenho (Observation and performance window)**: Na modelagem estatística, o modelo tenta prever o evento antecipadamente e não no momento, para que exista algum tempo de buffer para trabalhar em ações corretivas. Por exemplo, uma pergunta de uma empresa de cartão de crédito seria, por exemplo, qual é a probabilidade de um determinado cliente deixar o padrão nos próximos 12 meses? Para que eu possa ligar para ele e oferecer descontos ou desenvolver minhas estratégias de cobrança de acordo.\n",
    "\n",
    "Para responder a essa pergunta, uma probabilidade de modelo padrão (ou scorecard comportamental em termos técnicos) precisa ser desenvolvida usando variáveis independentes dos últimos 24 meses e uma variável dependente dos próximos 12 meses. Depois de preparar os dados com as variáveis $X$ e $Y$, eles serão divididos em 70% a 30% como dados de treinamento e teste aleatoriamente; esse método é chamado de validação em tempo, pois as amostras de trem e teste são do mesmo período:\n",
    "\n",
    "![16](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/f6494458-0b3d-4a84-81cd-f0b8e437076d.png)\n",
    "\n",
    "* **Validação em tempo e fora de tempo (In-time and out-of-time validation)**: a validação em tempo implica obter um conjunto de dados de treinamento e teste do mesmo período de tempo, enquanto a validação fora de tempo implica conjuntos de dados de treinamento e teste extraídos de diferentes períodos de tempo. Geralmente, o modelo apresenta desempenho pior na validação fora do prazo, em vez de dentro do prazo devido à razão óbvia de que as características dos conjuntos de dados de trem e teste podem diferir.\n",
    "\n",
    "* **R Quadrado e coeficiente de determinação (R-squared and coefficient of determination)**: é a medida da porcentagem da variação da variável resposta que é explicada por um modelo. É também uma medida de quão bem o modelo minimiza o erro em comparação com apenas utilizar a média como uma estimativa. Em alguns casos extremos, o quadrado R também pode ter um valor menor que zero, o que significa que os valores previstos do modelo têm desempenho pior do que apenas tomar a média simples como uma previsão para todas as observações. Estudaremos esse parâmetro em detalhes nos próximos capítulos:\n",
    "\n",
    "![17](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/09c49ce0-d8ad-45a6-bf34-783233f10b7e.jpg)\n",
    "\n",
    "![18](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/6dde425e-e8bb-4ec2-a009-ed711270a928.jpg)\n",
    "\n",
    "* **R Quadrado ajustado (Adjusted R-squared)**: A explicação da estatística do quadrado R ajustado é quase a mesma do quadrado R, mas penaliza o valor do quadrado R se variáveis adicionais sem uma forte correlação forem incluídas no modelo:\n",
    "\n",
    "![19](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/4cbf5330-fb5f-4700-978a-4cf05a63f142.jpg)\n",
    "\n",
    "Aqui, R2 = valor do quadrado R da amostra, n = tamanho da amostra, k = número de preditores (ou) variáveis.\n",
    "\n",
    "O valor do quadrado R ajustado é a métrica chave na avaliação da qualidade das regressões lineares. Qualquer modelo de regressão linear com o valor de R2 ajustado> = 0,7 é considerado um modelo suficientemente bom para implementar.\n",
    "\n",
    "Exemplo: o valor do quadrado R de uma amostra é 0,5, com um tamanho de amostra 50 e as variáveis independentes são 10 em número. R-quadrado ajustado calculado:\n",
    "\n",
    "![20](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/5e37964d-9ba7-4b55-9da1-69279791114a.jpg)\n",
    "\n",
    "* **Estimativa de máxima verossimilhança MLE (Maximum likelihood estimate)**: Estima os valores dos parâmetros de um modelo estatístico (regressão logística, para ser mais preciso), localizando os valores dos parâmetros que maximizam a probabilidade de realização das observações. Abordaremos esse método com mais profundidade no Capítulo 3, Regressão logística versus floresta aleatória.\n",
    "\n",
    "* **Critério de informação de Akaike AIC (Akaike information criteria)**: É usado na regressão logística, que é semelhante ao princípio do quadrado R ajustado para a regressão linear. Ele mede a qualidade relativa de um modelo para um determinado conjunto de dados:\n",
    "\n",
    "![21](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d4c56b6e-6aeb-4987-b221-b084abaabc1c.jpg)\n",
    "\n",
    "Aqui, k = número de preditores ou variáveis\n",
    "\n",
    "A idéia da AIC é penalizar a função objetivo se variáveis adicionais sem fortes habilidades preditivas forem incluídas no modelo. Este é um tipo de regularização em regressão logística.\n",
    "\n",
    "* **Entropia (Entropy)**: Isso vem da teoria da informação e é a medida da impureza nos dados. Se a amostra for completamente homogênea, a entropia for zero e se a amostra for igualmente dividida, ela terá uma entropia de 1. Nas árvores de decisão, o preditor com maior heterogeneidade será considerado o mais próximo ao nó raiz para classificar os dados fornecidos em classes em um modo ganancioso. Abordaremos esse tópico com mais profundidade no Capítulo 4, Modelos de aprendizado de máquina com base em árvore:\n",
    "\n",
    "![22](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/5a6d4f5c-b370-4e4f-ac25-32c2c12456cb.jpg)\n",
    "\n",
    "Aqui, n = número de classes. A entropia é máxima no meio, com o valor de 1 e mínima nos extremos como 0. Um valor baixo de entropia é desejável, pois segregará melhor as classes:\n",
    "\n",
    "![23](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/0de2fe80-c66b-4e2e-996d-14501cdbf9d7.png)\n",
    "\n",
    "Exemplo: Dados dois tipos de moedas em que o primeiro é justo (probabilidades de 1/2 cabeça e 1/2 cauda) e o outro é tendencioso (probabilidades de 1/3 cabeça e 2/3 de cauda), calcule o entropia para ambos e justifique qual é o melhor com relação à modelagem:\n",
    "\n",
    "![24](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/b11aa4c9-286c-45fb-974b-45362717cdf5.jpg)\n",
    "![25](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/6e905ac4-201d-4657-8c4d-c9e7ed03e145.jpg)\n",
    "\n",
    "De ambos os valores, o algoritmo da árvore de decisão escolhe a moeda tendenciosa em vez da moeda justa como um divisor de observação, devido ao fato de o valor da entropia ser menor.\n",
    "\n",
    "* **Ganho de informações (Information gain)**: é a redução esperada na entropia causada pelo particionamento dos exemplos de acordo com um determinado atributo. A idéia é começar com classes mistas e manter o particionamento até que cada nó atinja suas observações da classe mais pura. Em cada estágio, a variável com ganho máximo de informações é escolhida de maneira gananciosa:\n",
    "\n",
    "Ganho de informação = Entropia do pai - soma (% ponderada * Entropia do filho)\n",
    "\n",
    "% Ponderada = Número de observações em particular filho / soma (observações em todos os nós filhos)\n",
    "\n",
    "* **Gini**: A impureza de Gini é uma medida de classificação incorreta, que se aplica em um contexto classificador de várias classes. Gini funciona quase da mesma forma que entropia, exceto que Gini é mais rápido em calcular:\n",
    "\n",
    "![26](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2a100040-7da4-4bc9-bd0b-fbcff855b9f2.jpg)\n",
    "\n",
    "Aqui, i = número de classes. A semelhança entre Gini e entropia é mostrada a seguir:\n",
    "\n",
    "![27](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/ba78a735-7442-4597-8926-d1f4c050a65a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compromisso de polarização vs variação (Bias versus variance trade-off)\n",
    "\n",
    "Cada modelo possui componentes de erro de polarização e variação, além de ruído branco. Viés e variação estão inversamente relacionados entre si; ao tentar reduzir um componente, o outro componente do modelo aumentará. A verdadeira arte reside em criar um bom ajuste, equilibrando ambos. O modelo ideal terá baixo viés e baixa variação.\n",
    "\n",
    "Os erros do componente de viés vêm de suposições errôneas no algoritmo de aprendizado subjacente. Um viés alto pode fazer com que um algoritmo perca as relações relevantes entre os recursos e as saídas de destino; esse fenômeno causa um problema de underfitting.\n",
    "\n",
    "Por outro lado, os erros do componente de variação vêm da sensibilidade à mudança no ajuste do modelo, até uma pequena mudança nos dados de treinamento; alta variação pode causar um problema de sobreajuste:\n",
    "\n",
    "![28](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/0dc22ca4-98f7-44a0-9a82-e7d1154702bb.jpg)\n",
    "\n",
    "Um exemplo de modelo de alto viés é a regressão logística ou linear, na qual o ajuste do modelo é meramente uma linha reta e pode ter um componente de erro alto devido ao fato de um modelo linear não conseguir aproximar bem os dados subjacentes.\n",
    "\n",
    "Um exemplo de modelo de alta variância é uma árvore de decisão, na qual o modelo pode criar muita curva distorcida como um ajuste, no qual mesmo uma pequena alteração nos dados de treinamento causará uma mudança drástica no ajuste da curva.\n",
    "\n",
    "No momento, os modelos mais avançados estão utilizando modelos de alta variação, como árvores de decisão e executando conjuntos sobre eles para reduzir os erros causados pela alta variação e, ao mesmo tempo, não comprometer o aumento de erros devido à componente de viés. O melhor exemplo dessa categoria é a floresta aleatória (decision trees), na qual muitas árvores de decisão serão cultivadas de forma independente e agrupadas para obter o melhor ajuste; abordaremos isso nos próximos capítulos:\n",
    "\n",
    "![29](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d904ee08-0577-4fc2-81d6-f85cfafa07dd.png)\n",
    "\n",
    "## Dados de treino e teste (Train and test data)\n",
    "\n",
    "Na prática, os dados geralmente são divididos aleatoriamente 70-30 ou 80-20 em conjuntos de dados de treinamento e teste, respectivamente, na modelagem estatística, na qual os dados de treinamento utilizados para a construção do modelo e sua eficácia serão verificados nos dados de teste:\n",
    "\n",
    "![30](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/bbb2a548-6dba-4d9f-a4f7-8f20d13422e0.png)\n",
    "\n",
    "No código a seguir, dividimos os dados originais em dados de treinamento e teste em 70% - 30%. Um ponto importante a considerar aqui é que definimos os valores iniciais para números aleatórios para repetir a amostragem aleatória toda vez que criamos as mesmas observações nos dados de treinamento e teste. A repetibilidade é muito necessária para reproduzir os resultados.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "22\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Train & Test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "original_data = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/mtcars.csv\")\n",
    "\n",
    "train_data,test_data = train_test_split(original_data,train_size = 0.7,random_state=42)\n",
    "\n",
    "print(len(original_data))\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminologia de aprendizado de máquina para construção e validação de modelos (Machine learning terminology for model building and validation)\n",
    "\n",
    "Parece haver uma analogia entre modelagem estatística e aprendizado de máquina que abordaremos em capítulos subseqüentes em profundidade. No entanto, uma visão rápida foi fornecida da seguinte forma: na modelagem estatística, a regressão linear com duas variáveis independentes está tentando ajustar o melhor plano com o mínimo de erros, enquanto no aprendizado de máquina as variáveis independentes foram convertidas no quadrado dos termos de erro (quadrado) garante que a função se torne convexa, o que aprimora a convergência mais rápida e também garante uma otimização global) e otimizada com base em valores de coeficiente, em vez de variáveis independentes:\n",
    "\n",
    "![31](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/840c0fbb-2cd9-48a1-84cd-f07e18ca507f.png)\n",
    "\n",
    "O aprendizado de máquina utiliza otimização para ajustar todos os parâmetros de vários algoritmos. Portanto, é uma boa idéia conhecer alguns conceitos básicos sobre otimização.\n",
    "\n",
    "Antes de entrar em descida gradiente, a introdução de funções convexas e não convexas é muito útil. Funções convexas são funções nas quais uma linha traçada entre dois pontos aleatórios da função também se encontra dentro da função, enquanto isso não é verdade para funções não convexas. É importante saber se a função é convexa ou não convexa devido ao fato de que nas funções convexas, o ideal local também é o ideal global, enquanto que nas funções não convexas, o ideal local não garante o ideal global:\n",
    "\n",
    "![32](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/276607a1-0841-4e7f-99ec-fce02b7f49d4.png)\n",
    "\n",
    "Parece um problema difícil? Uma solução seria iniciar um processo de pesquisa em diferentes locais aleatórios; ao fazer isso, geralmente converge para o ideal global:\n",
    "\n",
    "* **Descida do gradiente (Gradient descent)**: Esta é uma maneira de minimizar a função objetivo $J(Θ)$ parametrizada pelo parâmetro do modelo $Θ ε R^d$ atualizando os parâmetros na direção oposta ao gradiente da função objetivo em relação aos parâmetros. A taxa de aprendizado determina o tamanho das medidas adotadas para atingir o mínimo.\n",
    "\n",
    "* **Descida de gradiente de lote completo (todas as observações de treinamento consideradas em cada iteração) - Full batch gradient descent (all training observations considered in each and every iteration)**: Na descida de gradiente de lote completo, todas as observações são consideradas para cada iteração; essa metodologia exige muita memória e também será lenta. Além disso, na prática, não precisamos ter todas as observações para atualizar os pesos. No entanto, esse método fornece a melhor maneira de atualizar parâmetros com menos ruído às custas de grandes cálculos.\n",
    "\n",
    "* **Descida do gradiente estocástico (uma observação por iteração) - Stochastic gradient descent (one observation per iteration)**: esse método atualiza os pesos fazendo uma observação em cada estágio da iteração. Este método fornece a maneira mais rápida de atravessar pesos; no entanto, há muito ruído durante a convergência.\n",
    "\n",
    "* **Mini descida de gradiente de lote (cerca de 30 observações de treinamento ou mais para cada iteração) - Mini batch gradient descent (about 30 training observations or more for each and every iteration)**: esse é um compromisso entre enormes custos computacionais e um método rápido de atualização de pesos. Nesse método, a cada iteração, cerca de 30 observações serão selecionadas aleatoriamente e gradientes calculados para atualizar os pesos do modelo. Aqui, uma pergunta que muitos podem fazer é: por que o mínimo 30 e não qualquer outro número? Se olharmos para o básico estatístico, 30 observações precisam ser consideradas para aproximar a amostra como uma população. No entanto, mesmo 40, 50 e assim por diante também se sairão bem na seleção de tamanho de lote. No entanto, um profissional precisa alterar o tamanho do lote e verificar os resultados, para determinar em qual valor o modelo está produzindo os melhores resultados:\n",
    "\n",
    "![33](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/26799de5-3ab2-4590-996b-dec37489d38d.png)\n",
    "\n",
    "## Regressão linear versus descida de gradiente (Linear regression versus gradient descent)\n",
    "\n",
    "No código a seguir, foi feita uma comparação entre a aplicação de regressão linear de maneira estatística e a descida de gradiente de maneira automática no mesmo conjunto de dados.\n",
    "\n",
    "\n",
    "\n",
    "Agora aplicaremos a descida gradiente do zero; em capítulos futuros, podemos usar os módulos internos do scikit-learn, em vez de fazê-lo a partir dos primeiros princípios. No entanto, aqui, uma ilustração foi fornecida sobre o funcionamento interno do método de otimização no qual todo o aprendizado de máquina foi construído.\n",
    "\n",
    "Definindo a função de descida do gradiente `gradient_descent` com o seguinte:\n",
    "\n",
    "* x: variável independente.\n",
    "* y: Variável dependente.\n",
    "* learn_rate: taxa de aprendizado com a qual os gradientes são atualizados; muito baixo causa convergência mais lenta e muito alto causa excesso de gradientes.\n",
    "* batch_size: número de observações consideradas em cada iteração para atualizar gradientes; um número alto causa um número menor de iterações e um número menor causa uma diminuição errática nos erros. Idealmente, o tamanho do lote deve ser um valor mínimo de 30 devido à significância estatística. No entanto, várias configurações precisam ser tentadas para verificar qual é a melhor.\n",
    "* max_iter: número máximo de iteração, além do qual o algoritmo será finalizado automaticamente:\n",
    "\n",
    "```python\n",
    "def gradient_descent(x, y,learn_rate, conv_threshold,batch_size, max_iter): \n",
    "    converged = False \n",
    "    iter = 0 \n",
    "    m = batch_size   \n",
    "    t0 = np.random.random(x.shape[1]) \n",
    "    t1 = np.random.random(x.shape[1]) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results\n",
      "Intercept 30.098860539622496 Coefficient [-0.06822828]\n",
      "Converged, iterations:  1142914\n",
      "Gradient Descent Results\n",
      "Intercept = [30.02495104] Coefficient = [-0.06781243]\n",
      "CPU times: user 5min 53s, sys: 140 ms, total: 5min 53s\n",
      "Wall time: 5min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Linear Regressio vs. Gradient Descent\n",
    "\n",
    "# The following code describes reading data using a pandas DataFrame:\n",
    "train_data = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/mtcars.csv\")\n",
    "\n",
    "# Converting DataFrame variables into NumPy arrays in order to process them in scikit learn \n",
    "# packages, as scikit-learn is built on NumPy arrays itself, is shown next:\n",
    "X = np.array(train_data[\"hp\"])  ; y = np.array(train_data[\"mpg\"])\n",
    "X = X.reshape(32,1); y = y.reshape(32,1)\n",
    "\n",
    "# mporting linear regression from the scikit-learn package; this works on the least squares method:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept = True)\n",
    "\n",
    "# Fitting a linear regression model on the data and display intercept and coefficient of \n",
    "# single variable (hp variable):\n",
    "model.fit(X,y)\n",
    "print (\"Linear Regression Results\")\n",
    "print (\"Intercept\",model.intercept_[0] ,\"Coefficient\",model.coef_[0])\n",
    "                   \n",
    "\n",
    "def gradient_descent(x, y,learn_rate, conv_threshold,batch_size,max_iter):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = batch_size \n",
    " \n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "\n",
    "    MSE = (sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])/ m)\n",
    "    \n",
    "    # O seguinte código afirma, execute o algoritmo até que ele não atenda aos \n",
    "    # critérios de convergência:\n",
    "    while not converged:\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)])\n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        temp0 = t0 - learn_rate * grad0\n",
    "        temp1 = t1 - learn_rate * grad1\n",
    "    \n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "        \n",
    "        # Calcule um novo erro com parâmetros atualizados, para verificar se o novo erro mudou mais que \n",
    "        # o valor do limite de convergência predefinido; caso contrário, pare as iterações e retorne \n",
    "        # os parâmetros:\n",
    "        MSE_New = (sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) / m)\n",
    "\n",
    "        if abs(MSE - MSE_New ) <= conv_threshold:\n",
    "            print ('Converged, iterations: ', iter)\n",
    "            converged = True\n",
    "    \n",
    "        MSE = MSE_New\n",
    "        iter += 1\n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print ('Max interactions reached')\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1\n",
    "\n",
    "#O código a seguir descreve a execução da função de descida de gradiente com valores definidos. \n",
    "# Taxa de aprendizado = 0,0003, limite de convergência = 1e-8, tamanho do lote = 32, número \n",
    "# máximo de iterações = 1500000:\n",
    "if __name__ == '__main__':\n",
    "    Inter, Coeff = gradient_descent(x = X,y = y,learn_rate=0.00003 ,conv_threshold=1e-8, batch_size=32,max_iter=1500000)\n",
    "    print (\"Gradient Descent Results\")\n",
    "    print (('Intercept = %s Coefficient = %s') %(Inter, Coeff)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perdas de aprendizado de máquina (Machine learning losses)\n",
    "\n",
    "A função de perda ou função de custo (loss function or cost function) no aprendizado de máquina é uma função que mapeia os valores das variáveis em um número real, representando intuitivamente algum custo associado aos valores das variáveis. Os métodos de otimização são aplicados para minimizar a função de perda, alterando os valores dos parâmetros, que é o tema central do aprendizado de máquina.\n",
    "\n",
    "A perda zero e um é L0-1 = 1 (m <= 0); na perda zero um, o valor da perda é 0 para m> = 0, enquanto 1 para m <0. A parte difícil dessa perda é que ela não é diferenciável, não convexa e também NP-difícil. Portanto, para tornar a otimização viável e solucionável, essas perdas são substituídas por diferentes perdas substitutas para diferentes problemas.\n",
    "\n",
    "As perdas substitutas usadas para o aprendizado de máquina no lugar da perda zero e um são fornecidas a seguir. A perda zero-um não é diferenciável, portanto, perdas aproximadas estão sendo usadas:\n",
    "\n",
    "* Perda ao quadrado (para regressão) [Squared loss (for regression)]\n",
    "* Perda de dobradiça (SVM) [Hinge loss (SVM)]\n",
    "* Logística / perda de log (regressão logística) [Logistic/log loss (logistic regression)]\n",
    "\n",
    "![34](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/97fa0538-9572-4d0c-a54e-543ff9a2de4e.png)\n",
    "\n",
    "Algumas funções de custo:\n",
    "\n",
    "![35](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/01c313b6-eb39-45b6-b40c-2fc3cc592660.jpg)\n",
    "![36](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/7a043c0c-6c26-4b83-bbb8-6037f5b941f8.jpg)\n",
    "![37](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/4cff1dee-a99f-46f0-ac65-0cb506dd5990.jpg)\n",
    "![38](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/37581cda-5d75-471c-b264-6599e4f735ce.jpg)\n",
    "![39](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/1b0b29d5-8b88-49fa-b35e-b9548fc0af7b.jpg)\n",
    "\n",
    "## Quando parar de ajustar os modelos de aprendizado de máquina (When to stop tuning machine learning models)\n",
    "\n",
    "Quando parar de ajustar os hiperparâmetros em um modelo de aprendizado de máquina é uma questão de um milhão de dólares. Esse problema pode ser resolvido principalmente acompanhando os erros de treinamento e teste. Ao aumentar a complexidade de um modelo, ocorrem os seguintes estágios:\n",
    "\n",
    "* Etapa 1: Etapa de ajuste insuficiente (Underfitting) - trem alto e altos erros de teste (ou trem baixo e baixa precisão do teste)\n",
    "* Etapa 2: Bom estágio de ajuste (cenário ideal) - baixo trem e baixos erros de teste (ou alto trem e alta precisão de teste)\n",
    "* Estágio 3: Estágio de sobreajuste (Overfitting) - baixo trem e altos erros de teste (ou trem alto e baixa precisão do teste)\n",
    "\n",
    "![40](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/56fd7a24-5694-4dcc-af1d-03c1461fb779.png)\n",
    "\n",
    "## Dados de treinamento, validação e teste (Train, validation, and test data)\n",
    "\n",
    "A validação cruzada (Cross-validation) não é popular no mundo da modelagem estatística por muitas razões; os modelos estatísticos são lineares e robustos, e não apresentam um problema de alta variação/super adaptação. Portanto, o ajuste do modelo permanecerá o mesmo nos dados de trem ou teste, o que não se aplica ao mundo do aprendizado de máquina. Além disso, na modelagem estatística, muitos testes são realizados no nível do parâmetro individual, além das métricas agregadas, enquanto no aprendizado de máquina não temos visibilidade no nível do parâmetro individual:\n",
    "\n",
    "![41](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/8894aa2e-cf66-4e8f-8a6b-52b93c540f98.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 ms, sys: 4 µs, total: 12.2 ms\n",
      "Wall time: 946 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train Validation Test split\n",
    "\n",
    "original_data = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/mtcars.csv\")\n",
    "\n",
    "def data_split(dat,trf = 0.5,vlf=0.25,tsf = 0.25):\n",
    "    nrows = dat.shape[0]\n",
    "    trnr = int(nrows*trf)\n",
    "    vlnr = int(nrows*vlf)\n",
    "    \n",
    "    # The following Python code splits the data into training and the remaining data. The remaining data will be further split into validation and test datasets:\n",
    "    tr_data,rmng = train_test_split(dat,train_size = trnr,random_state=42)\n",
    "    vl_data, ts_data = train_test_split(rmng,train_size = vlnr,random_state=45)\n",
    "    \n",
    "    return (tr_data,vl_data,ts_data)\n",
    "\n",
    "# Implementation of the split function on the original data to create three datasets (by 50 percent, 25 percent, and 25 percent splits) is as follows:\n",
    "train_data, validation_data, test_data = data_split(original_data,trf=0.5,vlf=0.25,tsf=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação cruzada (Cross-validation)\n",
    "\n",
    "A validação cruzada é outra maneira de garantir robustez no modelo às custas da computação. Na metodologia de modelagem comum, um modelo é desenvolvido com dados de trem e avaliado em dados de teste. Em alguns casos extremos, o treinamento e o teste podem não ter sido selecionados de maneira homogênea e alguns casos extremos não vistos podem aparecer nos dados do teste, o que reduzirá o desempenho do modelo.\n",
    "\n",
    "Por outro lado, na metodologia de validação cruzada, os dados foram divididos em partes iguais e o treinamento foi realizado em todas as outras partes dos dados, exceto em uma parte, na qual o desempenho será avaliado. Esse processo repetiu quantas peças o usuário escolheu.\n",
    "\n",
    "Exemplo: na validação cruzada cinco vezes, os dados serão divididos em cinco partes, posteriormente treinados em quatro partes dos dados e testados em uma parte dos dados. Este processo será executado cinco vezes, a fim de cobrir todos os pontos nos dados. Por fim, o erro calculado será a média de todos os erros:\n",
    "\n",
    "![42](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/f79f8d66-3ae7-48a5-a418-b6433c5c14e2.png)\n",
    "\n",
    "## Grid search\n",
    "\n",
    "A pesquisa em grade no aprendizado de máquina é uma maneira popular de ajustar os hiperparâmetros do modelo para encontrar a melhor combinação para determinar o melhor ajuste:\n",
    "\n",
    "![43](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/ac3f2f5a-9199-4bb7-8ce6-47e4dc307a0e.png)\n",
    "\n",
    "No código a seguir, a implementação foi realizada para determinar se um usuário específico clicará em um anúncio ou não. A pesquisa em grade foi implementada usando um classificador de árvore de decisão para fins de classificação. Os parâmetros de ajuste são a profundidade da árvore, o número mínimo de observações no nó terminal e o número mínimo de observações necessárias para executar a divisão do nó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score: \n",
      " 0.9699346405228759\n",
      "\n",
      " Best parameters set: \n",
      "\n",
      "\tclf__max_depth: 100\n",
      "\tclf__min_samples_leaf: 2\n",
      "\tclf__min_samples_split: 2\n",
      "\n",
      " Confusion Matrix on Test data \n",
      " [[816  17]\n",
      " [ 24 127]]\n",
      "\n",
      " Test Accuracy \n",
      " 0.9583333333333334\n",
      "\n",
      "Precision Recall f1 table \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       833\n",
      "           1       0.88      0.84      0.86       151\n",
      "\n",
      "    accuracy                           0.96       984\n",
      "   macro avg       0.93      0.91      0.92       984\n",
      "weighted avg       0.96      0.96      0.96       984\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    5.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Grid search on Decision Trees\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "input_data = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/ad.csv\",header=None)\n",
    "\n",
    "X_columns = set(input_data.columns.values)\n",
    "y = input_data[len(input_data.columns.values)-1]\n",
    "X_columns.remove(len(input_data.columns.values)-1)\n",
    "X = input_data[list(X_columns)]\n",
    "\n",
    "# Split the data into train and testing:\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,random_state=33)\n",
    "\n",
    "# Create a pipeline to create combinations of variables for the grid search:\n",
    "pipeline = Pipeline([\n",
    "    ('clf', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "\n",
    "# Combinations to explore are given as parameters in Python dictionary format:\n",
    "parameters = {\n",
    "    'clf__max_depth': (50,100,150),\n",
    "    'clf__min_samples_split': (2, 3),\n",
    "    'clf__min_samples_leaf': (1, 2, 3)\n",
    "}\n",
    "\n",
    "# The n_jobs field is for selecting the number of cores in a computer; -1 means it uses all the \n",
    "# cores in the computer. The scoring methodology is accuracy, in which many other options \n",
    "# can be chosen, such as precision, recall, and f1:\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the best parameters of grid search:\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print ('\\n Best score: \\n', grid_search.best_score_)\n",
    "print ('\\n Best parameters set: \\n')\n",
    "\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print (\"\\n Confusion Matrix on Test data \\n\",confusion_matrix(y_test,y_pred))\n",
    "print (\"\\n Test Accuracy \\n\",accuracy_score(y_test,y_pred))\n",
    "print (\"\\nPrecision Recall f1 table \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
