{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"width: 100%; background-color: #F7F7F7; margin-bottom: 3em;\">\n",
    "    <h1 style=\"font-family: courier; color: #AF4545; font-size: 4em; padding-top: .5em;\"><b>Statistical fundamentals and terminology for model building and validation</b></h1>\n",
    "    <img src=\"../img/statistics-head.png\">    \n",
    "</div>\n",
    "\n",
    "Este estudo é baseado no livro `Statistics for Machine Learning`, e o código abaixo possui trechos integrais do livro. Dito isso existem algumas adaptações, sugestões e complementos que jugo necessários.\n",
    "\n",
    "---\n",
    "\n",
    "Par iniciar vou importar os pacotes necessários na primeira parte do estudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é uma introdução básica, a estatística é um tema para uma vida quanto mais um notebook. Com isso em mente, vou me ater aos principais pontos, os necessários para um bom conhecimento do universo do `Machine Learning`.\n",
    "\n",
    "> A análise preditiva se baseia na suposição de que a história tende a se repetir.\n",
    "\n",
    "Neste caso, para fazermos predições, nosso objetivo será ajustar um **modelo matemático** aos dados históricos validando suas principais medidas, de tal maneira que este modelo possa ser utilizado a fim de prever eventos futuros baseados nas mesmas **variáveis explicativas** que estudamos no passado.\n",
    "\n",
    "Modelos estatísticos são uma classe de modelos matemáticos que geralmente são especificados por equações matemáticas que relacionam uma ou mais variáveis a fim de se aproximar a realidade. Pressupostos incorporados por modelos estatísticos descrevem um conjunto de distribuições de probabilidade, que o diferencia dos modelos não estatísticos, matemáticos ou de aprendizado de máquina.\n",
    "\n",
    "Os modelos estatísticos sempre começam com algumas suposições subjacentes para as quais todas as variáveis devem se manter; então, o desempenho fornecido pelo modelo é estatisticamente significativo. Portanto, conhecer os vários bits envolvidos em todos os blocos de construção fornece uma base sólida para ser um bom cientista de dados.\n",
    "\n",
    "* População: é a totalidade, a lista completa de observações ou todos os pontos de dados sobre o assunto em estudo.\n",
    "* Amostra: uma amostra é um subconjunto de uma população, geralmente uma pequena parte da população que está sendo analisada.\n",
    "\n",
    "Geralmente, é caro realizar uma análise em uma população inteira; portanto, a maioria dos métodos estatísticos trata de tirar conclusões sobre uma população analisando uma amostra.\n",
    "\n",
    "* **Parâmetro versus estatística**: qualquer medida calculada na população é um parâmetro, enquanto que em uma amostra é chamada de estatística.\n",
    "* **Média**: Esta é uma média aritmética simples, calculada pela soma agregada dos valores dividida por uma contagem desses valores. A média é sensível aos valores discrepantes nos dados. Um valor externo é o valor de um conjunto ou coluna que é altamente desviado de muitos outros valores nos mesmos dados; geralmente tem valores muito altos ou baixos.\n",
    "* **Mediana**: é o ponto médio dos dados e é calculado organizando-os em ordem crescente ou decrescente. Se houver N observações.\n",
    "* **Moda**: é o ponto mais repetitivo nos dados:\n",
    "\n",
    "![0](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d7ca0b32-24a6-43fd-8281-3f995895ff1b.png)\n",
    "\n",
    "O código abaixo usa o pacote stats e numpy para calular a média (median), mediana (median) e moda (mode).\n",
    "\n",
    "A matriz inicial foi criada utilizando o numpy, isso dado que os modelos estatísticos utilizados neste estudo serão criados utilizando o pacote scikit-learn, que é baseado no numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 4.33\n",
      "Median : 4.0\n",
      "Mode : 2\n"
     ]
    }
   ],
   "source": [
    "data = np.array([4,5,1,2,7,2,6,9,3])\n",
    "\n",
    "# Calculate Mean\n",
    "dt_mean = np.mean(data)\n",
    "print (\"Mean :\",round(dt_mean,2))\n",
    "              \n",
    "# Calculate Median                 \n",
    "dt_median = np.median(data)\n",
    "print (\"Median :\",dt_median)\n",
    "\n",
    "# Calculate Mode                     \n",
    "dt_mode =  stats.mode(data)\n",
    "print (\"Mode :\",dt_mode[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuições de probabilidade\n",
    "\n",
    "As principais distribuições de probabilidade estão implementadas na biblioteca `scipy.stats`. Todas as distribuições implementadas nesta biblioteca tem um conjunto rico de métodos os principais são:\n",
    "\n",
    "  - pdf: função densidade probabilidade ou função de probabilidade.\n",
    "  - cdf: função de distribuição acumulada.\n",
    "  - sf: função de sobreviência (complementar da cdf).\n",
    "  - ppf: função quantil (inversa da cdf).\n",
    "  - isf: inversa da função de sobreviência (complementar do inverso da cdf).\n",
    "  - stats: esperança, variância, assimetria (skew) e curtose.\n",
    "  - moment: Momentos não centrais.\n",
    "  - rvs: amostras aleatórias.\n",
    "\n",
    "Interessante notar que funções como a pdf e cdf são definidas na reta real, mesmo que a distribuição não corresponda a este suporte. Neste caso as funções vão retornar zero no caso da pdf e 0 ou 1 no caso da cdf. Por exemplo, o suporta da distribuição beta é o intervalo aberto (0,1). Assim, se a pdf fora deste intervalo será 0 e a cdf abaixo de 0 será 0 e acima de 1 será 1. \n",
    "\n",
    "Outro aspecto interessante da forma como a biblioteca `scipy.stats` implementa as distribuições é que você pode usá-las de duas formas: a primeira é a chamada forma congelada onde você inicializa a distribuição com os parâmetros de interesse e depois usa. A segunda você aplica a função diretamente passando como argumentos o ponto e os parâmetros para a avaliação da função. Vamos ver um exemplo,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08801633169107488\n",
      "0.08801633169107488\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as sp\n",
    "\n",
    "print(sp.norm.pdf(5, 3, 4)) # Avalia a distribuição Normal com mu = 3 e sigma2 = 4 no ponto 5.\n",
    "\n",
    "mydist = sp.norm(3, 4) # Modo frozen \n",
    "print(mydist.pdf(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca `scipy.stats` é muito rica em distribuições e a documentação é muito detalhada recomendo que veja o site da biblioteca [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html). Um outro aspecto útil é que todas as distribuições contínuas estão implementadas como um modelo de locação e escala. Esta é uma abordagem um pouco não usual, mas uma grande vantagem é que sabemos a parametrização de todas as distribuições contínuas previamente. A lista de distribuições é muito extensa e inclui distribuições multivariadas como a Gaussiana, Dirichlet e Wishart. \n",
    "\n",
    "Como um exemplo para explorar como usamos as distribuições da `scipy.stats` vou usar em um primeiro momento a distribuição Gaussiana porque suas propriedades são bem conhecidas. Em um segundo momento fazer usar uma distribuição não usual como a *generalized extreme value distribution*. Vamos calcular algumas quantidades da distribuição Gaussiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 25.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "my_norm = sp.norm(loc = 10, scale = 5)\n",
    "# Aspectos da distribuição\n",
    "my_norm.expect() # Esperança\n",
    "my_norm.median() # Mediana\n",
    "my_norm.moment(n = 1) # Primeiro momento (esperança)\n",
    "my_norm.moment(n = 2) # Segundo momento\n",
    "my_norm.stats() # Média e variancia\n",
    "my_norm.std() # Erro padrão\n",
    "my_norm.var() # Variance\n",
    "my_norm.entropy() # Entropia\n",
    "\n",
    "mean, var, skew, kurt = my_norm.stats(moments = 'mvsk')\n",
    "print(mean, var, skew, kurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda não falamos sobre gráficos em python, mas eu vou inicializar uma figura, mais detalhes serão vistos nos próximos encontros. Gráfico da função densidade probabilidade.\n",
    "\n",
    "Vamos simular uma amostra aleatório da distribuição Gaussiana e fazer um histograma e sobrepor a densidade que acabamos de calcular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "x = np.linspace(my_norm.ppf(0.01), my_norm.ppf(0.99), 100)\n",
    "ax.plot(x, my_norm.pdf(x), 'r-', lw=1, alpha=0.6, label='norm pdf')\n",
    "\n",
    "amostra = my_norm.rvs(10000)\n",
    "ax.hist(amostra, density=True, histtype='stepfilled', alpha=0.2)\n",
    "ax.legend(loc='best', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro método que também está disponível para todas as distribuições implementadas na biblioteca `scipy.stats` é o método `fit`. Como o nome sugere este método ajusta a distribuição para um vetor de observações. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.677558586317708, 4.577823988903881)\n",
      "(3518998.448550458, 9.677857227098563, 4.577722231657916)\n",
      "(9.769109923281029, 2.9145346025200185)\n"
     ]
    }
   ],
   "source": [
    "amostra = my_norm.rvs(100)\n",
    "print(sp.norm.fit(amostra)) # Ajustando a própria Normal\n",
    "print(sp.t.fit(amostra)) # Ajustando a distribuição t\n",
    "print(sp.cauchy.fit(amostra)) # Cauchy ou qq outra que seja adequada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em geral as distribuições de probabilidade implementadas na biblioteca `scipy.stats` são vetorizadas. Assim, se o argumento for um objeto `ndarray` a função será aplicada em cada uma das entradas do `ndarray` e a saída também será um `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00443185, 0.05399097, 0.24197072, 0.39894228, 0.24197072,\n",
       "       0.05399097, 0.00443185])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([-3,-2,-1,0,1,2,3])\n",
    "sp.norm.pdf(a, loc = 0, scale = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim termino esta rápida introdução de como e onde as funções de probabilidades estão implementadas em python. Na sequência vou introduzir as principais idéias de otimização de funções em python.\n",
    "\n",
    "## Otimização\n",
    "\n",
    "Otimização de funções tem um papel central em inferência estatística. Uma vez que o estimador de maxima verossimilhança é o supremo da função de log-verossimilhança encontrar este ponto é crucial para o processo de inferência. O python através do modulo `scipy.optimization` fornece vários algoritmos para numericamente encontrar o mínimo/máximo de uma função pré-especificada. Este modulo também fornece algoritmos para solução de sistemas de equações não-lineares e minimização via métodos dos mínimos quadrados. Vamos ver alguns exemplos de como usar esta poderosa biblioteca. Como exemplo vou implementar a log-verossimilhança de um modelo de regressão linear simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.22747985 2.98913084 3.96304652 3.88286941 5.25055912 5.22025798\n",
      " 7.005927   7.79190839 7.51322383 9.67606474]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(range(0,10))\n",
    "par = np.array([2,0.8,0.5])\n",
    "my_norm = sp.norm(loc = 0, scale = par[2])\n",
    "e = my_norm.rvs(len(x))\n",
    "mu = par[0] + par[1]*x\n",
    "y = mu + e\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.08325619  0.77083923 -0.78114441]\n",
      "6.377595779854763\n",
      "[ 2.08325397  0.77084288 -0.78117857]\n",
      "6.377595760422819\n",
      "[ 2.08325332  0.77084298 -0.78117893]\n",
      "6.377595760417703\n",
      "[ 2.08321123  0.77084964 -0.78129439]\n",
      "6.377595905871967\n",
      "[-5.96046448e-07 -2.38418579e-06  4.76837158e-07]\n",
      "[[ 7.22133604e-02 -1.14451524e-02 -3.26751095e-04]\n",
      " [-1.14451524e-02  2.54060362e-03 -3.12294568e-05]\n",
      " [-3.26751095e-04 -3.12294568e-05  4.70887117e-02]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from math import exp, log\n",
    "\n",
    "def linreg(par, y, x):\n",
    "    mu = par[0] + par[1]*x\n",
    "    output = -sp.norm.logpdf(y, loc = mu, scale = exp(par[2])).sum()\n",
    "    #print(output)\n",
    "    return(output)\n",
    "\n",
    "# Avaliando a log-lik no ponto\n",
    "linreg(par = np.array([2.1788,0.7743, log(0.3817)]), y = y, x = x) \n",
    "\n",
    "# Valores iniciais\n",
    "par = np.array([2,0.8, log(2)])\n",
    "\n",
    "# Nelder-Mead\n",
    "res1 = minimize(linreg, par, method='nelder-mead', args = (y,x))\n",
    "print(res1.x)\n",
    "print(res1.fun)\n",
    "\n",
    "# Gradiente Conjugado\n",
    "res2 = minimize(linreg, par, method = 'CG', args = (y,x))\n",
    "print(res2.x)\n",
    "print(res2.fun)\n",
    "\n",
    "# BFGS\n",
    "res3 = minimize(linreg, par, method = 'BFGS',  args = (y,x))\n",
    "print(res3.x)\n",
    "print(res3.fun)\n",
    "\n",
    "# Powell\n",
    "res4 = minimize(linreg, par, method = 'Powell', args = (y,x))\n",
    "print(res4.x)\n",
    "print(res4.fun)\n",
    "\n",
    "# Escore\n",
    "print(res3.jac)\n",
    "\n",
    "# Temos o inverso do hessiano\n",
    "print(res3.hess_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Medida de dispersão ou variabilidade: Dispersão é a variação nos dados e mede as inconsistências no valor das variáveis nos dados. A dispersão, na verdade, fornece uma idéia sobre o spread, em vez de valores centrais.\n",
    "* Faixa: Essa é a diferença entre o máximo e o mínimo do valor.\n",
    "* Variância: é a média dos desvios ao quadrado da média ($xi$ = pontos de dados, $µ$ = média dos dados, $N$ = número de pontos de dados). A dimensão da variação é o quadrado dos valores reais. A razão para usar o denominador $N-1$ para uma amostra em vez de $N$ na população é devido ao grau de liberdade. 1 grau de liberdade perdido em uma amostra no momento do cálculo da variação é devido à extração da substituição da amostra:\n",
    "\n",
    "![1](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/04829f06-fff4-4694-9b3e-62508ac4a609.jpg)\n",
    "\n",
    "* Desvio padrão: esta é a raiz quadrada da variação. Aplicando a raiz quadrada na variação, medimos a dispersão em relação à variável original e não ao quadrado da dimensão:\n",
    "\n",
    "![2](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/ba61b3ca-aaf4-48cb-98a8-6f7acd61fef8.jpg)\n",
    "\n",
    "* Quantiles: Estes são simplesmente fragmentos idênticos dos dados. Os quantiles cobrem percentis, decis, quartis e assim por diante. Essas medidas são calculadas após a organização dos dados em ordem crescente:\n",
    "    * Percentil: nada mais é do que a porcentagem de pontos de dados abaixo do valor dos dados completos originais. A mediana é o 50º percentil, pois o número de pontos de dados abaixo da mediana é de cerca de 50% dos dados.\n",
    "    * Decil: este é o 10º percentil, o que significa que o número de pontos de dados abaixo do decil é 10% de todos os dados.\n",
    "    * Quartil: este é um quarto dos dados e também o 25º percentil. O primeiro quartil é 25% dos dados, o segundo quartil é 50% dos dados, o terceiro quartil é 75% dos dados. O segundo quartil também é conhecido como mediana ou percentil 50 ou quinto decil.\n",
    "    * Intervalo interquartil: é a diferença entre o terceiro quartil e o primeiro quartil. É eficaz na identificação de valores discrepantes nos dados. O intervalo interquartil descreve os 50% do meio dos pontos de dados.\n",
    "\n",
    "![3](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d2d6d978-262e-41fb-a731-41f8b8384d9b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample variance: 400\n",
      "Sample std.dev: 20.0\n",
      "Range: 69\n"
     ]
    }
   ],
   "source": [
    "# Deviance calculations\n",
    "\n",
    "from statistics import variance, stdev\n",
    "\n",
    "game_points = np.array([35,56,43,59,63,79,35,41,64,43,93,60,77,24,82])\n",
    "\n",
    "# Calculate Variance\n",
    "dt_var = variance(game_points)\n",
    "print (\"Sample variance:\", round(dt_var,2))\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "dt_std = stdev(game_points)\n",
    "print (\"Sample std.dev:\",round(dt_std,2))\n",
    "               \n",
    "# Calculate Range\n",
    "dt_rng = np.max(game_points,axis=0) - np.min(game_points,axis=0)\n",
    "print (\"Range:\",dt_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles:\n",
      "20% 39.800000000000004\n",
      "80% 77.4\n",
      "100% 93.0\n",
      "Inter quartile range: 28.5\n"
     ]
    }
   ],
   "source": [
    "#Calculate percentiles\n",
    "print (\"Quantiles:\")\n",
    "\n",
    "for val in [20,80,100]:\n",
    "    dt_qntls = np.percentile(game_points,val) \n",
    "    print (str(val)+\"%\" ,dt_qntls)\n",
    "                                \n",
    "# Calculate IQR                           \n",
    "q75, q25 = np.percentile(game_points, [75 ,25])\n",
    "print (\"Inter quartile range:\",q75-q25 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Teste de hipótese: Este é o processo de fazer inferências sobre a população em geral, realizando alguns testes estatísticos em uma amostra. Hipóteses nulas e alternativas são formas de validar se uma suposição é estatisticamente significativa ou não.\n",
    "* Valor p: a probabilidade de obter um resultado estatístico de teste é pelo menos tão extrema quanto a que foi realmente observada, assumindo que a hipótese nula seja verdadeira (geralmente na modelagem, em relação a cada variável independente, um valor p menor que 0,05 é considerados significativos e maiores que 0,05 são considerados insignificantes; no entanto, esses valores e definições podem mudar em relação ao contexto).\n",
    "\n",
    "As etapas envolvidas no teste de hipóteses são as seguintes:\n",
    "\n",
    "1. Suponha uma hipótese nula (geralmente sem diferença, sem significância, e assim por diante; uma hipótese nula sempre tenta assumir que não há padrão de anomalia e é sempre homogêneo, e assim por diante).\n",
    "2. Colete a amostra.\n",
    "3. Calcule as estatísticas de teste da amostra para verificar se a hipótese é estatisticamente significativa ou não.\n",
    "4. Decida aceitar ou rejeitar a hipótese nula com base na estatística do teste.\n",
    "\n",
    "* Exemplo de teste de hipótese: Um fabricante de chocolate que também é seu amigo afirma que todos os chocolates produzidos em sua fábrica pesam pelo menos 1.000 ge você tem uma sensação engraçada de que isso pode não ser verdade; vocês dois coletaram uma amostra de 30 chocolates e descobriram que o peso médio de chocolate é de 990 g, com desvio padrão da amostra de 12,5 g. Dado o nível de significância de 0,05, podemos rejeitar a reivindicação feita por seu amigo?\n",
    "\n",
    "A hipótese nula é que $µ0 ≥ 1000$ (todos os chocolates pesam mais de 1.000 g).\n",
    "\n",
    "Amostra coletada:\n",
    "\n",
    "![4](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/aa23714a-a55f-48ae-8103-22298c05c866.jpg)\n",
    "\n",
    "Teste estatístico:\n",
    "\n",
    "![5](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/696917cc-a126-4477-9e7d-df420f914a90.jpg)\n",
    "\n",
    "$t = (990 - 1000) / (12,5 / sqrt (30)) = - 4,3818$\n",
    "\n",
    "Valor t crítico das tabelas $t$ = $t0,05, 30 = 1,699 => - t0,05, 30 = -1,699$\n",
    "\n",
    "Valor P = 7,03 e-05\n",
    "\n",
    "A estatística do teste é -4,3818, que é menor que o valor crítico de -1,699. Portanto, podemos rejeitar a hipótese nula (afirmação de seu amigo) de que o peso médio de um chocolate está acima de 1.000 g.\n",
    "\n",
    "Além disso, outra maneira de decidir a reivindicação é usar o valor-p. Um valor p menor que 0,05 significa que os valores reivindicados e os valores médios da distribuição são significativamente diferentes; portanto, podemos rejeitar a hipótese nula:\n",
    "\n",
    "![6](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/fd9dea23-5c21-49c8-b363-623f43621e61.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic: -4.38\n",
      "Critical value from t-table: -1.699\n",
      "Lower tail p-value from t-table 7.035025729010886e-05\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis testing        \n",
    "\n",
    "xbar = 990; mu0 = 1000; s = 12.5; n = 30\n",
    "\n",
    "# Test Statistic\n",
    "t_smple  = (xbar-mu0)/(s/np.sqrt(float(n)))\n",
    "print (\"Test Statistic:\",round(t_smple,2))\n",
    "\n",
    "# Critical value from t-table\n",
    "alpha = 0.05\n",
    "t_alpha = stats.t.ppf(alpha,n-1)\n",
    "print (\"Critical value from t-table:\",round(t_alpha,3))\n",
    "\n",
    "#Lower tail p-value from t-table                        \n",
    "p_val = stats.t.sf(np.abs(t_smple), n-1)\n",
    "print (\"Lower tail p-value from t-table\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Erro tipo I e II: O teste de hipóteses geralmente é realizado nas amostras e não em toda a população, devido às restrições práticas dos recursos disponíveis para coletar todos os dados disponíveis. No entanto, a realização de inferências sobre a população a partir de amostras tem seus próprios custos, como rejeitar bons resultados ou aceitar resultados falsos, sem mencionar separadamente, quando aumentos no tamanho da amostra levam a minimizar erros do tipo I e II:\n",
    "    * Erro tipo I: rejeitando uma hipótese nula quando verdadeira\n",
    "    * Erro tipo II: aceitando uma hipótese nula quando falsa\n",
    "* Distribuição normal: isso é muito importante na estatística por causa do teorema do limite central, que afirma que a população de todas as amostras possíveis de tamanho n de uma população com μ2 e variação σ2 próximas de uma distribuição normal:\n",
    "\n",
    "![7](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/f2dd7f75-6394-4238-9d6b-9fe7f1f52c31.jpg)\n",
    "\n",
    "Exemplo: Suponha que as notas dos testes de um exame de admissão se ajustem a uma distribuição normal. Além disso, a pontuação média no teste é 52 e o desvio padrão é 16,3. Qual é a porcentagem de alunos com 67 ou mais notas no exame?\n",
    "\n",
    "![08](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/0e2e6317-da80-44bf-80f6-4df886b67803.jpg)\n",
    "![09](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/59ccd175-1fd1-4cc5-87b9-e0ebf4023c9b.jpg)\n",
    "![10](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d1d90969-0e9d-4d4f-8381-4f8a1b634ee3.jpg)\n",
    "![11](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/11cf9613-e4b5-4bac-8056-1f1a0967030f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob. to score more than 67 is  17.87 %\n"
     ]
    }
   ],
   "source": [
    "# Normal Distribution\n",
    "\n",
    "xbar = 67; mu0 = 52; s = 16.3\n",
    "\n",
    "# Calculating z-score\n",
    "z = (67-52)/16.3\n",
    "\n",
    "# Calculating probability under the curve    \n",
    "p_val = 1- stats.norm.cdf(z)\n",
    "\n",
    "print (\"Prob. to score more than 67 is \",round(p_val*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui-quadrado: Esse teste de independência é um dos testes de hipótese mais básicos e comuns na análise estatística de dados categóricos. Dadas duas variáveis aleatórias categóricas $X$ e $Y$, o teste do qui-quadrado da independência determina se existe ou não uma dependência estatística entre elas.\n",
    "\n",
    "O teste geralmente é realizado calculando $χ2$ a partir dos dados e $χ2$ com (m-1, n-1) graus da tabela. É tomada uma decisão sobre se as duas variáveis são independentes com base no valor real e no valor da tabela, o que for maior:\n",
    "\n",
    "![11](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/c77913b4-32d4-4cf5-9474-a9e54f7837cc.jpg)\n",
    "\n",
    "Exemplo: Na tabela a seguir, calcule se o hábito de fumar tem impacto no comportamento do exercício:\n",
    "\n",
    "![12](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/1abc30fb-a6df-4091-aa10-2647446f98eb.png)\n",
    "\n",
    "Ao criar uma tabela usando a função de **crosstab** (tabela de referência cruzada), obteremos os campos de totais de linha e coluna extra. No entanto, para criar a tabela observada, precisamos extrair a parte das variáveis e ignorar os totais.\n",
    "\n",
    "```python\n",
    "observed = survey_tab.ix[0:4,0:3]  \n",
    "```\n",
    "\n",
    "A função chi2_contingency do pacote stats usa a tabela observada e subsequentemente calcula sua tabela esperada, seguida pelo cálculo do valor-p para verificar se duas variáveis são dependentes ou não. Se o valor de p for < 0,05, existe uma forte dependência entre duas variáveis, enquanto que, se o valor de p for > 0,05, não há dependência entre as variáveis:\n",
    "\n",
    "```python\n",
    "contg = stats.chi2_contingency(observed= observed) \n",
    "p_value = round(contg[1],3)\n",
    "```\n",
    "O valor de p é 0.483, o que siginifica que não há dependência entre o hábito de fumar e o comportamento do exercício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value is:  0.483\n"
     ]
    }
   ],
   "source": [
    "# Chi-square independence test\n",
    "import pandas as pd\n",
    "\n",
    "survey = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/survey.csv\")\n",
    "\n",
    "# Tabulating 2 variables with row & column variables respectively\n",
    "survey_tab = pd.crosstab(survey.Smoke, survey.Exer, margins = True)\n",
    "\n",
    "# Creating observed table for analysis\n",
    "observed = survey_tab.iloc[0:4,0:3] \n",
    "\n",
    "contg = stats.chi2_contingency(observed= observed)\n",
    "p_value = round(contg[1],3)\n",
    "\n",
    "print (\"P-value is: \",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANOVA: A análise de variância testa a hipótese de que as médias de duas ou mais populações são iguais. As ANOVAs avaliam a importância de um ou mais fatores comparando as médias das variáveis de resposta nos diferentes níveis de fatores. A hipótese nula afirma que todas as médias populacionais são iguais, enquanto a hipótese alternativa afirma que pelo menos uma é diferente.\n",
    "\n",
    "Exemplo: Uma empresa de fertilizantes desenvolveu três novos tipos de fertilizantes universais após pesquisas que podem ser utilizadas para cultivar qualquer tipo de colheita. Para descobrir se os três têm um rendimento similar, eles escolheram aleatoriamente seis tipos de cultivo no estudo. De acordo com o delineamento em blocos ao acaso, cada tipo de cultura será testado com os três tipos de fertilizante separadamente. A tabela a seguir representa o rendimento em $g/m^2$. No nível de significância de 0,05, teste se os rendimentos médios para os três novos tipos de fertilizantes são todos iguais:\n",
    "\n",
    "| Fertilizer 1| Fertilizer 2 | Fertilizer 3 |\n",
    "| --- | --- | --- |\n",
    "| 62 | 54 | 48 |\n",
    "| 62 | 56 | 62 |\n",
    "| 90 | 58 | 92 |\n",
    "| 42 | 36 | 96 |\n",
    "| 84 | 72 | 92 |\n",
    "| 64 | 34 | 80 |\n",
    "\n",
    "Resultado: o valor de p foi menor que 0,05, portanto, podemos rejeitar a hipótese nula de que a produção média dos fertilizantes é igual. Os fertilizantes fazem uma diferença significativa para as culturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic : 3.66 , p-value : 0.051\n"
     ]
    }
   ],
   "source": [
    "#ANOVA\n",
    "\n",
    "fetilizers = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/fetilizers.csv\")\n",
    "\n",
    "one_way_anova = stats.f_oneway(fetilizers[\"fertilizer1\"], fetilizers[\"fertilizer2\"], fetilizers[\"fertilizer3\"])\n",
    "\n",
    "print (\"Statistic :\", round(one_way_anova[0],2),\", p-value :\",round(one_way_anova[1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de confusão: esta é a matriz do real versus o previsto. Este conceito é melhor explicado com o exemplo de previsão de câncer usando o modelo:\n",
    "\n",
    "![13](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/f512942f-24dd-47b7-8b28-f9ce86376587.png)\n",
    "\n",
    "Alguns termos usados em uma matriz de confusão são:\n",
    "\n",
    "* Verdadeiros positivos (TPs): Verdadeiros positivos são casos em que predizemos a doença e sim quando o paciente realmente tem a doença.\n",
    "* Negativos verdadeiros (TNs): Casos em que predizemos a doença como não, quando o paciente realmente não tem a doença.\n",
    "* Falsos positivos (PFs): quando prevemos a doença como sim, quando o paciente realmente não tem a doença. Os FPs também são considerados erros do tipo I.\n",
    "* Falsos negativos (SN): quando prevemos a doença como não, quando o paciente realmente a tem. Os FNs também são considerados erros do tipo II.\n",
    "* Precisão (P): Quando o previsto é sim, com que frequência é correto?\n",
    "\n",
    "$(TP / TP + FP)$\n",
    "\n",
    "Recall (R) / sensibilidade / taxa positiva verdadeira: Entre os sim reais, que fração foi prevista como sim?\n",
    "\n",
    "$(TP / TP + FN)$\n",
    "\n",
    "* Pontuação F1 (F1): Esta é a média harmônica da precisão e recuperação. Multiplicar a constante de 2 escala a pontuação para 1 quando a precisão e a recuperação são 1:\n",
    "\n",
    "![14](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/41e2c94c-d6c9-4227-81ac-d806f07c64f4.jpg)\n",
    "\n",
    "Especificidade (Specificity): Entre os números reais, que fração foi prevista como não? Também equivalente a 1 - taxa de falsos positivos:\n",
    "\n",
    "$(TN / TN + FP)$\n",
    "\n",
    "Área sob curva (ROC): a curva característica operacional do receptor é usada para traçar entre a taxa positiva verdadeira (TPR) e a taxa positiva falsa (FPR), também conhecido como gráfico de sensibilidade e 1- especificidade:\n",
    "\n",
    "![15](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/97f53598-dd30-406c-9106-b193d6844666.png)\n",
    "\n",
    "A área sob curva é utilizada para definir o limiar da probabilidade de corte para classificar a probabilidade prevista em várias classes; abordaremos como esse método funciona nos próximos capítulos.\n",
    "\n",
    "* Janela de observação e desempenho (Observation and performance window): Na modelagem estatística, o modelo tenta prever o evento antecipadamente e não no momento, para que exista algum tempo de buffer para trabalhar em ações corretivas. Por exemplo, uma pergunta de uma empresa de cartão de crédito seria, por exemplo, qual é a probabilidade de um determinado cliente deixar o padrão nos próximos 12 meses? Para que eu possa ligar para ele e oferecer descontos ou desenvolver minhas estratégias de cobrança de acordo.\n",
    "\n",
    "Para responder a essa pergunta, uma probabilidade de modelo padrão (ou scorecard comportamental em termos técnicos) precisa ser desenvolvida usando variáveis independentes dos últimos 24 meses e uma variável dependente dos próximos 12 meses. Depois de preparar os dados com as variáveis $X$ e $Y$, eles serão divididos em 70% a 30% como dados de treinamento e teste aleatoriamente; esse método é chamado de validação em tempo, pois as amostras de trem e teste são do mesmo período:\n",
    "\n",
    "![16](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/f6494458-0b3d-4a84-81cd-f0b8e437076d.png)\n",
    "\n",
    "* Validação em tempo e fora de tempo (In-time and out-of-time validation): a validação em tempo implica obter um conjunto de dados de treinamento e teste do mesmo período de tempo, enquanto a validação fora de tempo implica conjuntos de dados de treinamento e teste extraídos de diferentes períodos de tempo. Geralmente, o modelo apresenta desempenho pior na validação fora do prazo, em vez de dentro do prazo devido à razão óbvia de que as características dos conjuntos de dados de trem e teste podem diferir.\n",
    "\n",
    "* R Quadrado e coeficiente de determinação (R-squared and coefficient of determination): é a medida da porcentagem da variação da variável resposta que é explicada por um modelo. É também uma medida de quão bem o modelo minimiza o erro em comparação com apenas utilizar a média como uma estimativa. Em alguns casos extremos, o quadrado R também pode ter um valor menor que zero, o que significa que os valores previstos do modelo têm desempenho pior do que apenas tomar a média simples como uma previsão para todas as observações. Estudaremos esse parâmetro em detalhes nos próximos capítulos:\n",
    "\n",
    "![17](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/09c49ce0-d8ad-45a6-bf34-783233f10b7e.jpg)\n",
    "\n",
    "![18](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/6dde425e-e8bb-4ec2-a009-ed711270a928.jpg)\n",
    "\n",
    "* R Quadrado ajustado (Adjusted R-squared): A explicação da estatística do quadrado R ajustado é quase a mesma do quadrado R, mas penaliza o valor do quadrado R se variáveis adicionais sem uma forte correlação forem incluídas no modelo:\n",
    "\n",
    "![19](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/4cbf5330-fb5f-4700-978a-4cf05a63f142.jpg)\n",
    "\n",
    "Aqui, R2 = valor do quadrado R da amostra, n = tamanho da amostra, k = número de preditores (ou) variáveis.\n",
    "\n",
    "O valor do quadrado R ajustado é a métrica chave na avaliação da qualidade das regressões lineares. Qualquer modelo de regressão linear com o valor de R2 ajustado> = 0,7 é considerado um modelo suficientemente bom para implementar.\n",
    "\n",
    "Exemplo: o valor do quadrado R de uma amostra é 0,5, com um tamanho de amostra 50 e as variáveis independentes são 10 em número. R-quadrado ajustado calculado:\n",
    "\n",
    "![20](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/5e37964d-9ba7-4b55-9da1-69279791114a.jpg)\n",
    "\n",
    "* Estimativa de máxima verossimilhança MLE (Maximum likelihood estimate): Estima os valores dos parâmetros de um modelo estatístico (regressão logística, para ser mais preciso), localizando os valores dos parâmetros que maximizam a probabilidade de realização das observações. Abordaremos esse método com mais profundidade no Capítulo 3, Regressão logística versus floresta aleatória.\n",
    "\n",
    "* Critério de informação de Akaike AIC (Akaike information criteria ): É usado na regressão logística, que é semelhante ao princípio do quadrado R ajustado para a regressão linear. Ele mede a qualidade relativa de um modelo para um determinado conjunto de dados:\n",
    "\n",
    "![21](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/d4c56b6e-6aeb-4987-b221-b084abaabc1c.jpg)\n",
    "\n",
    "Aqui, k = número de preditores ou variáveis\n",
    "\n",
    "A idéia da AIC é penalizar a função objetivo se variáveis adicionais sem fortes habilidades preditivas forem incluídas no modelo. Este é um tipo de regularização em regressão logística.\n",
    "\n",
    "* Entropia (Entropy): Isso vem da teoria da informação e é a medida da impureza nos dados. Se a amostra for completamente homogênea, a entropia for zero e se a amostra for igualmente dividida, ela terá uma entropia de 1. Nas árvores de decisão, o preditor com maior heterogeneidade será considerado o mais próximo ao nó raiz para classificar os dados fornecidos em classes em um modo ganancioso. Abordaremos esse tópico com mais profundidade no Capítulo 4, Modelos de aprendizado de máquina com base em árvore:\n",
    "\n",
    "![22](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/5a6d4f5c-b370-4e4f-ac25-32c2c12456cb.jpg)\n",
    "\n",
    "Aqui, n = número de classes. A entropia é máxima no meio, com o valor de 1 e mínima nos extremos como 0. Um valor baixo de entropia é desejável, pois segregará melhor as classes:\n",
    "\n",
    "![23](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/0de2fe80-c66b-4e2e-996d-14501cdbf9d7.png)\n",
    "\n",
    "Exemplo: Dados dois tipos de moedas em que o primeiro é justo (probabilidades de 1/2 cabeça e 1/2 cauda) e o outro é tendencioso (probabilidades de 1/3 cabeça e 2/3 de cauda), calcule o entropia para ambos e justifique qual é o melhor com relação à modelagem:\n",
    "\n",
    "![24](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/b11aa4c9-286c-45fb-974b-45362717cdf5.jpg)\n",
    "![25](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/6e905ac4-201d-4657-8c4d-c9e7ed03e145.jpg)\n",
    "\n",
    "De ambos os valores, o algoritmo da árvore de decisão escolhe a moeda tendenciosa em vez da moeda justa como um divisor de observação, devido ao fato de o valor da entropia ser menor.\n",
    "\n",
    "* Ganho de informações (Information gain): é a redução esperada na entropia causada pelo particionamento dos exemplos de acordo com um determinado atributo. A idéia é começar com classes mistas e manter o particionamento até que cada nó atinja suas observações da classe mais pura. Em cada estágio, a variável com ganho máximo de informações é escolhida de maneira gananciosa:\n",
    "\n",
    "Ganho de informação = Entropia do pai - soma (% ponderada * Entropia do filho)\n",
    "\n",
    "% Ponderada = Número de observações em particular filho / soma (observações em todos os nós filhos)\n",
    "\n",
    "* Gini: A impureza de Gini é uma medida de classificação incorreta, que se aplica em um contexto classificador de várias classes. Gini funciona quase da mesma forma que entropia, exceto que Gini é mais rápido em calcular:\n",
    "\n",
    "![26](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2a100040-7da4-4bc9-bd0b-fbcff855b9f2.jpg)\n",
    "\n",
    "Aqui, i = número de classes. A semelhança entre Gini e entropia é mostrada a seguir:\n",
    "\n",
    "![27](https://learning.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/ba78a735-7442-4597-8926-d1f4c050a65a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "22\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Train & Test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "original_data = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/mtcars.csv\")\n",
    "\n",
    "train_data,test_data = train_test_split(original_data,train_size = 0.7,random_state=42)\n",
    "\n",
    "print(len(original_data))\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results\n",
      "Intercept 30.098860539622496 Coefficient [-0.06822828]\n",
      "Converged, iterations:  1140566\n",
      "Gradient Descent Results\n",
      "Intercept = [30.02495106] Coefficient = [-0.06781243]\n"
     ]
    }
   ],
   "source": [
    "# Linear Regressio vs. Gradient Descent\n",
    "\n",
    "train_data = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/mtcars.csv\")\n",
    "                        \n",
    "X = np.array(train_data[\"hp\"])  ; y = np.array(train_data[\"mpg\"])\n",
    "X = X.reshape(32,1); y = y.reshape(32,1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept = True)\n",
    " \n",
    "model.fit(X,y)\n",
    "print (\"Linear Regression Results\")\n",
    "print (\"Intercept\",model.intercept_[0] ,\"Coefficient\",model.coef_[0])\n",
    "                   \n",
    "\n",
    "def gradient_descent(x, y,learn_rate, conv_threshold,batch_size,max_iter):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = batch_size \n",
    " \n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "\n",
    "    MSE = (sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])/ m)\n",
    "\n",
    "    while not converged:\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)])\n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        temp0 = t0 - learn_rate * grad0\n",
    "        temp1 = t1 - learn_rate * grad1\n",
    "    \n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        MSE_New = (sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) / m)\n",
    "\n",
    "        if abs(MSE - MSE_New ) <= conv_threshold:\n",
    "            print ('Converged, iterations: ', iter)\n",
    "            converged = True\n",
    "    \n",
    "        MSE = MSE_New\n",
    "        iter += 1\n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print ('Max interactions reached')\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Inter, Coeff = gradient_descent(x = X,y = y,learn_rate=0.00003 ,conv_threshold=1e-8, batch_size=32,max_iter=1500000)\n",
    "    print (\"Gradient Descent Results\")\n",
    "    print (('Intercept = %s Coefficient = %s') %(Inter, Coeff)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation Test split\n",
    "\n",
    "original_data = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/mtcars.csv\")\n",
    "\n",
    "def data_split(dat,trf = 0.5,vlf=0.25,tsf = 0.25):\n",
    "    nrows = dat.shape[0]\n",
    "    trnr = int(nrows*trf)\n",
    "    vlnr = int(nrows*vlf)\n",
    "    \n",
    "    tr_data,rmng = train_test_split(dat,train_size = trnr,random_state=42)\n",
    "    vl_data, ts_data = train_test_split(rmng,train_size = vlnr,random_state=45)\n",
    "    \n",
    "    return (tr_data,vl_data,ts_data)\n",
    "\n",
    "train_data, validation_data, test_data = data_split(original_data,trf=0.5,vlf=0.25,tsf=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score: \n",
      " 0.9699346405228759\n",
      "\n",
      " Best parameters set: \n",
      "\n",
      "\tclf__max_depth: 100\n",
      "\tclf__min_samples_leaf: 2\n",
      "\tclf__min_samples_split: 2\n",
      "\n",
      " Confusion Matrix on Test data \n",
      " [[816  17]\n",
      " [ 24 127]]\n",
      "\n",
      " Test Accuracy \n",
      " 0.9583333333333334\n",
      "\n",
      "Precision Recall f1 table \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       833\n",
      "           1       0.88      0.84      0.86       151\n",
      "\n",
      "    accuracy                           0.96       984\n",
      "   macro avg       0.93      0.91      0.92       984\n",
      "weighted avg       0.96      0.96      0.96       984\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    5.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Grid search on Decision Trees\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "input_data = pd.read_csv(\"https://meriatdatasets.blob.core.windows.net/public/ad.csv\",header=None)\n",
    "\n",
    "X_columns = set(input_data.columns.values)\n",
    "y = input_data[len(input_data.columns.values)-1]\n",
    "X_columns.remove(len(input_data.columns.values)-1)\n",
    "X = input_data[list(X_columns)]\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,random_state=33)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "parameters = {\n",
    "    'clf__max_depth': (50,100,150),\n",
    "    'clf__min_samples_split': (2, 3),\n",
    "    'clf__min_samples_leaf': (1, 2, 3)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print ('\\n Best score: \\n', grid_search.best_score_)\n",
    "print ('\\n Best parameters set: \\n')\n",
    "\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "print (\"\\n Confusion Matrix on Test data \\n\",confusion_matrix(y_test,y_pred))\n",
    "print (\"\\n Test Accuracy \\n\",accuracy_score(y_test,y_pred))\n",
    "print (\"\\nPrecision Recall f1 table \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
